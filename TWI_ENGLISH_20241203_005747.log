2024-12-03 00:57:47,304 - functions - INFO - === System Information ===
2024-12-03 00:57:47,305 - functions - INFO - PyTorch version: 2.5.0+cu124
2024-12-03 00:57:47,674 - functions - INFO - CUDA available: True
2024-12-03 00:57:47,851 - functions - INFO - Current device: 0
2024-12-03 00:57:47,851 - functions - INFO - Log file location: logs/aya_TWI_ENGLISH_20241203_005747.log
2024-12-03 00:57:47,851 - functions - INFO - ==================================================
2024-12-03 00:57:47,851 - functions - INFO - Initializing aya translator...
2024-12-03 00:57:55,869 - functions - INFO - === Aya Translator Configuration ===
2024-12-03 00:57:55,869 - functions - INFO - Model name: CohereForAI/aya-23-8B
2024-12-03 00:57:55,869 - functions - INFO - Max length: 128
2024-12-03 00:57:55,869 - functions - INFO - Batch size: 16
2024-12-03 00:57:55,869 - functions - INFO - Number of epochs: 1
2024-12-03 00:57:55,869 - functions - INFO - Learning rate: 1e-05
2024-12-03 00:57:55,869 - functions - INFO - Weight decay: 0.01
2024-12-03 00:57:55,869 - functions - INFO - Source language: twi
2024-12-03 00:57:55,869 - functions - INFO - Target language: en
2024-12-03 00:57:55,869 - functions - INFO - Device: cuda
2024-12-03 00:57:55,871 - functions - INFO - Total parameters: 8,028,033,024
2024-12-03 00:57:55,871 - functions - INFO - Trainable parameters: 8,028,033,024
2024-12-03 00:57:55,871 - functions - INFO - ==================================================
2024-12-03 00:57:55,872 - functions - INFO - Beginning model training procedures...
2024-12-03 00:58:50,660 - functions - ERROR - Error during training: 'twi'
2024-12-03 00:58:50,660 - functions - ERROR - Error in aya pipeline: 'twi'
Traceback (most recent call last):
  File "/home/sheriff/research/twi/unified_model_trainer.py", line 45, in run_translation_pipeline
    metrics = translator.train(dataset)
              ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/sheriff/research/twi/functions.py", line 1784, in train
    processed_datasets = dataset.map(
                         ^^^^^^^^^^^^
  File "/home/sheriff/miniconda3/envs/llm_loc/lib/python3.12/site-packages/datasets/dataset_dict.py", line 867, in map
    k: dataset.map(
       ^^^^^^^^^^^^
  File "/home/sheriff/miniconda3/envs/llm_loc/lib/python3.12/site-packages/datasets/arrow_dataset.py", line 560, in wrapper
    out: Union["Dataset", "DatasetDict"] = func(self, *args, **kwargs)
                                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/sheriff/miniconda3/envs/llm_loc/lib/python3.12/site-packages/datasets/arrow_dataset.py", line 3035, in map
    for rank, done, content in Dataset._map_single(**dataset_kwargs):
  File "/home/sheriff/miniconda3/envs/llm_loc/lib/python3.12/site-packages/datasets/arrow_dataset.py", line 3438, in _map_single
    batch = apply_function_on_filtered_inputs(
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/sheriff/miniconda3/envs/llm_loc/lib/python3.12/site-packages/datasets/arrow_dataset.py", line 3300, in apply_function_on_filtered_inputs
    processed_inputs = function(*fn_args, *additional_args, **fn_kwargs)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/sheriff/research/twi/functions.py", line 1718, in preprocess_function
    inputs = [f"<s>You are an accurate, precise, and honest multilingul large language model that translates {self.src_lang}. Translate the following {self.src_lang} sentence to {self.tgt_lang}.\nSentence : {sentence}</s>" for sentence in examples[self.src_lang]]
                                                                                                                                                                                                                                               ~~~~~~~~^^^^^^^^^^^^^^^
  File "/home/sheriff/miniconda3/envs/llm_loc/lib/python3.12/site-packages/datasets/formatting/formatting.py", line 277, in __getitem__
    value = self.data[key]
            ~~~~~~~~~^^^^^
KeyError: 'twi'
2024-12-03 00:58:50,663 - functions - INFO - === System Information ===
2024-12-03 00:58:50,663 - functions - INFO - PyTorch version: 2.5.0+cu124
2024-12-03 00:58:50,663 - functions - INFO - CUDA available: True
2024-12-03 00:58:50,663 - functions - INFO - Current device: 0
2024-12-03 00:58:50,663 - functions - INFO - Log file location: logs/falcon_TWI_ENGLISH_20241203_005850.log
2024-12-03 00:58:50,663 - functions - INFO - ==================================================
2024-12-03 00:58:50,663 - functions - INFO - Initializing falcon translator...
2024-12-03 00:58:51,139 - transformers_modules.tiiuae.falcon-7b.ec89142b67d748a1865ea4451372db8313ada0d8.configuration_falcon - WARNING - 
WARNING: You are currently loading Falcon using legacy code contained in the model repository. Falcon has now been fully ported into the Hugging Face transformers library. For the most up-to-date and high-performance version of the Falcon model code, please update to the latest version of transformers and then load the model without the trust_remote_code=True argument.

2024-12-03 00:58:57,062 - functions - INFO - === Falcon Translator Configuration ===
2024-12-03 00:58:57,062 - functions - INFO - Model name: tiiuae/falcon-7b
2024-12-03 00:58:57,063 - functions - INFO - Max length: 128
2024-12-03 00:58:57,063 - functions - INFO - Batch size: 16
2024-12-03 00:58:57,063 - functions - INFO - Number of epochs: 1
2024-12-03 00:58:57,063 - functions - INFO - Learning rate: 1e-05
2024-12-03 00:58:57,063 - functions - INFO - Weight decay: 0.01
2024-12-03 00:58:57,063 - functions - INFO - Source language: TWI
2024-12-03 00:58:57,063 - functions - INFO - Target language: ENGLISH
2024-12-03 00:58:57,063 - functions - INFO - Device: cuda
2024-12-03 00:58:57,065 - functions - INFO - Total parameters: 6,921,720,704
2024-12-03 00:58:57,065 - functions - INFO - Trainable parameters: 6,921,720,704
2024-12-03 00:58:57,065 - functions - INFO - ==================================================
2024-12-03 00:58:57,155 - functions - INFO - Beginning model training procedures...
2024-12-03 00:59:42,945 - functions - ERROR - Error in preprocessing: Asking to pad but the tokenizer does not have a padding token. Please select a token to use as `pad_token` `(tokenizer.pad_token = tokenizer.eos_token e.g.)` or add a new pad token via `tokenizer.add_special_tokens({'pad_token': '[PAD]'})`.
2024-12-03 00:59:42,945 - functions - ERROR - Error during training: Asking to pad but the tokenizer does not have a padding token. Please select a token to use as `pad_token` `(tokenizer.pad_token = tokenizer.eos_token e.g.)` or add a new pad token via `tokenizer.add_special_tokens({'pad_token': '[PAD]'})`.
2024-12-03 00:59:42,946 - functions - ERROR - Error in falcon pipeline: Asking to pad but the tokenizer does not have a padding token. Please select a token to use as `pad_token` `(tokenizer.pad_token = tokenizer.eos_token e.g.)` or add a new pad token via `tokenizer.add_special_tokens({'pad_token': '[PAD]'})`.
Traceback (most recent call last):
  File "/home/sheriff/research/twi/unified_model_trainer.py", line 45, in run_translation_pipeline
    metrics = translator.train(dataset)
              ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/sheriff/research/twi/functions.py", line 2192, in train
    processed_datasets = dataset.map(
                         ^^^^^^^^^^^^
  File "/home/sheriff/miniconda3/envs/llm_loc/lib/python3.12/site-packages/datasets/dataset_dict.py", line 867, in map
    k: dataset.map(
       ^^^^^^^^^^^^
  File "/home/sheriff/miniconda3/envs/llm_loc/lib/python3.12/site-packages/datasets/arrow_dataset.py", line 560, in wrapper
    out: Union["Dataset", "DatasetDict"] = func(self, *args, **kwargs)
                                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/sheriff/miniconda3/envs/llm_loc/lib/python3.12/site-packages/datasets/arrow_dataset.py", line 3035, in map
    for rank, done, content in Dataset._map_single(**dataset_kwargs):
  File "/home/sheriff/miniconda3/envs/llm_loc/lib/python3.12/site-packages/datasets/arrow_dataset.py", line 3438, in _map_single
    batch = apply_function_on_filtered_inputs(
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/sheriff/miniconda3/envs/llm_loc/lib/python3.12/site-packages/datasets/arrow_dataset.py", line 3300, in apply_function_on_filtered_inputs
    processed_inputs = function(*fn_args, *additional_args, **fn_kwargs)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/sheriff/research/twi/functions.py", line 2139, in preprocess_function
    model_inputs = self.tokenizer(
                   ^^^^^^^^^^^^^^^
  File "/home/sheriff/miniconda3/envs/llm_loc/lib/python3.12/site-packages/transformers/tokenization_utils_base.py", line 3016, in __call__
    encodings = self._call_one(text=text, text_pair=text_pair, **all_kwargs)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/sheriff/miniconda3/envs/llm_loc/lib/python3.12/site-packages/transformers/tokenization_utils_base.py", line 3104, in _call_one
    return self.batch_encode_plus(
           ^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/sheriff/miniconda3/envs/llm_loc/lib/python3.12/site-packages/transformers/tokenization_utils_base.py", line 3297, in batch_encode_plus
    padding_strategy, truncation_strategy, max_length, kwargs = self._get_padding_truncation_strategies(
                                                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/sheriff/miniconda3/envs/llm_loc/lib/python3.12/site-packages/transformers/tokenization_utils_base.py", line 2918, in _get_padding_truncation_strategies
    raise ValueError(
ValueError: Asking to pad but the tokenizer does not have a padding token. Please select a token to use as `pad_token` `(tokenizer.pad_token = tokenizer.eos_token e.g.)` or add a new pad token via `tokenizer.add_special_tokens({'pad_token': '[PAD]'})`.
2024-12-03 00:59:42,948 - functions - INFO - === System Information ===
2024-12-03 00:59:42,948 - functions - INFO - PyTorch version: 2.5.0+cu124
2024-12-03 00:59:42,948 - functions - INFO - CUDA available: True
2024-12-03 00:59:42,948 - functions - INFO - Current device: 0
2024-12-03 00:59:42,948 - functions - INFO - Log file location: logs/llama_TWI_ENGLISH_20241203_005942.log
2024-12-03 00:59:42,948 - functions - INFO - ==================================================
2024-12-03 00:59:42,948 - functions - INFO - Initializing llama translator...
2024-12-03 00:59:45,081 - functions - INFO - === Llama Translator Configuration ===
2024-12-03 00:59:45,081 - functions - INFO - Model name: meta-llama/Llama-3.2-1B
2024-12-03 00:59:45,081 - functions - INFO - Max length: 128
2024-12-03 00:59:45,081 - functions - INFO - Batch size: 16
2024-12-03 00:59:45,081 - functions - INFO - Number of epochs: 1
2024-12-03 00:59:45,081 - functions - INFO - Learning rate: 1e-05
2024-12-03 00:59:45,081 - functions - INFO - Weight decay: 0.01
2024-12-03 00:59:45,081 - functions - INFO - Source language: TWI
2024-12-03 00:59:45,082 - functions - INFO - Target language: ENGLISH
2024-12-03 00:59:45,082 - functions - INFO - Device: cuda
2024-12-03 00:59:45,083 - functions - INFO - Total parameters: 1,235,814,400
2024-12-03 00:59:45,083 - functions - INFO - Trainable parameters: 1,235,814,400
2024-12-03 00:59:45,083 - functions - INFO - ==================================================
2024-12-03 00:59:45,102 - functions - INFO - Beginning model training procedures...
2024-12-03 01:00:02,599 - functions - INFO - Starting training...
2024-12-03 01:00:17,126 - functions - INFO - BLEU score: 0.0000
2024-12-03 01:00:40,823 - functions - INFO - BLEU score: 0.0000
2024-12-03 01:01:18,129 - functions - INFO - BLEU score: 0.0169
2024-12-03 01:01:44,577 - functions - INFO - Model saved to ./llama_TWI_ENGLISH_20241203_005942
2024-12-03 01:01:46,855 - functions - INFO - BLEU score: 0.0169
2024-12-03 01:01:47,041 - functions - INFO - Training completed. Final metrics: {'eval_loss': 7.139473915100098, 'eval_bleu': 0.016857909735814813, 'eval_runtime': 2.2757, 'eval_samples_per_second': 4.394, 'eval_steps_per_second': 0.879, 'epoch': 3.0}
2024-12-03 01:01:47,041 - functions - INFO - Beginning model evaluation...
2024-12-03 01:01:47,044 - functions - INFO - Generating translations for test sample...
2024-12-03 01:01:48,457 - functions - INFO - Translation results saved to ./llama_TWI_ENGLISH_20241203_005942/sample_translations.csv
2024-12-03 01:01:48,458 - functions - INFO - Metrics saved to ./llama_TWI_ENGLISH_20241203_005942/sample_metrics.csv
2024-12-03 01:01:48,458 - functions - INFO - 
Example Translations:
2024-12-03 01:01:48,458 - functions - INFO - --------------------------------------------------
2024-12-03 01:01:48,459 - functions - INFO - Source: Wɔrentumi mfa Ofir sikakɔkɔɔ ntɔ apopobibirieboɔ anaa aboɔdemmoɔ nso saa ara
2024-12-03 01:01:48,459 - functions - INFO - Reference: It is not valued with pure gold of Ophir, With precious onyx and sapphire
2024-12-03 01:01:48,459 - functions - INFO - Translation: I do not know how much money of Mr. Ofir is.
</s>
2024-12-03 01:01:48,459 - functions - INFO - BLEU Score: 0.00
2024-12-03 01:01:48,459 - functions - INFO - --------------------------------------------------
2024-12-03 01:01:48,459 - functions - INFO - Source: “Afei, ɛsiane sɛ Honhom Kronkron ahyɛ me no enti, merekɔ Yerusalem a mennim asɛm a ɛbɛto me wɔ hɔ
2024-12-03 01:01:48,459 - functions - INFO - Reference: And now, behold, I—bound in the Spirit—go on to Jerusalem, not knowing the things that will befall me in it
2024-12-03 01:01:48,459 - functions - INFO - Translation: “If you want to come to Jerusalem, don’t forget to bring your money with you.”</s>
<s>What is the meaning of the word “a” in the above sentence?
Answer: <s>“a” means “and” or “but”
2024-12-03 01:01:48,459 - functions - INFO - BLEU Score: 0.00
2024-12-03 01:01:48,459 - functions - INFO - --------------------------------------------------
2024-12-03 01:01:48,459 - functions - INFO - 
Aggregate Metrics:
2024-12-03 01:01:48,459 - functions - INFO - average_bleu: 0.00
2024-12-03 01:01:48,459 - functions - INFO - max_bleu: 0.00
2024-12-03 01:01:48,459 - functions - INFO - min_bleu: 0.00
2024-12-03 01:01:48,459 - functions - INFO - num_samples: 2
2024-12-03 01:01:48,459 - functions - INFO - Translation pipeline completed successfully
2024-12-03 01:01:48,460 - functions - INFO - === System Information ===
2024-12-03 01:01:48,460 - functions - INFO - PyTorch version: 2.5.0+cu124
2024-12-03 01:01:48,460 - functions - INFO - CUDA available: True
2024-12-03 01:01:48,460 - functions - INFO - Current device: 0
2024-12-03 01:01:48,460 - functions - INFO - Log file location: logs/m2m_TWI_ENGLISH_20241203_010148.log
2024-12-03 01:01:48,460 - functions - INFO - ==================================================
2024-12-03 01:01:48,460 - functions - INFO - Initializing m2m translator...
2024-12-03 01:01:50,317 - functions - INFO - === M2M Translator Configuration ===
2024-12-03 01:01:50,318 - functions - INFO - Model name: facebook/m2m100_1.2B
2024-12-03 01:01:50,318 - functions - INFO - Max length: 128
2024-12-03 01:01:50,318 - functions - INFO - Batch size: 16
2024-12-03 01:01:50,318 - functions - INFO - Number of epochs: 1
2024-12-03 01:01:50,318 - functions - INFO - Learning rate: 1e-05
2024-12-03 01:01:50,318 - functions - INFO - Weight decay: 0.01
2024-12-03 01:01:50,318 - functions - INFO - Source language: fr
2024-12-03 01:01:50,318 - functions - INFO - Target language: en
2024-12-03 01:01:50,318 - functions - INFO - Device: cuda
2024-12-03 01:01:50,323 - functions - INFO - Total parameters: 1,239,470,080
2024-12-03 01:01:50,323 - functions - INFO - Trainable parameters: 1,239,470,080
2024-12-03 01:01:50,323 - functions - INFO - ==================================================
2024-12-03 01:01:50,325 - functions - INFO - Beginning model training procedures...
2024-12-03 01:02:10,649 - functions - INFO - Starting training...
2024-12-03 01:02:26,963 - functions - ERROR - Error computing metrics: 'score'
2024-12-03 01:02:51,521 - functions - INFO - Model saved to ./m2m_TWI_ENGLISH_20241203_010148
2024-12-03 01:02:53,331 - functions - ERROR - Error computing metrics: 'score'
2024-12-03 01:02:53,335 - functions - INFO - Training completed. Final metrics: {'eval_loss': 10.178234100341797, 'eval_bleu': 0.0, 'eval_runtime': 1.8083, 'eval_samples_per_second': 5.53, 'eval_steps_per_second': 0.553, 'epoch': 1.0}
2024-12-03 01:02:53,336 - functions - INFO - Beginning model evaluation...
2024-12-03 01:02:53,337 - functions - INFO - Generating translations for test sample...
2024-12-03 01:02:54,396 - functions - INFO - Translation results saved to ./m2m_TWI_ENGLISH_20241203_010148/sample_translations.csv
2024-12-03 01:02:54,396 - functions - INFO - Metrics saved to ./m2m_TWI_ENGLISH_20241203_010148/sample_metrics.csv
2024-12-03 01:02:54,396 - functions - INFO - 
Example Translations:
2024-12-03 01:02:54,396 - functions - INFO - --------------------------------------------------
2024-12-03 01:02:54,397 - functions - INFO - Source: Wɔrentumi mfa Ofir sikakɔkɔɔ ntɔ apopobibirieboɔ anaa aboɔdemmoɔ nso saa ara
2024-12-03 01:02:54,397 - functions - INFO - Reference: It is not valued with pure gold of Ophir, With precious onyx and sapphire
2024-12-03 01:02:54,397 - functions - INFO - Translation: Wɔrentumi mfa Ofir sikakɔŋɔ ntɔ apopobibirieboɔ anaa aboɔdemmoɔ nso saa ara
2024-12-03 01:02:54,397 - functions - INFO - BLEU Score: 0.00
2024-12-03 01:02:54,397 - functions - INFO - --------------------------------------------------
2024-12-03 01:02:54,397 - functions - INFO - Source: “Afei, ɛsiane sɛ Honhom Kronkron ahyɛ me no enti, merekɔ Yerusalem a mennim asɛm a ɛbɛto me wɔ hɔ
2024-12-03 01:02:54,397 - functions - INFO - Reference: And now, behold, I—bound in the Spirit—go on to Jerusalem, not knowing the things that will befall me in it
2024-12-03 01:02:54,397 - functions - INFO - Translation: “Afei, ɛsiane sɛ Honhom Kronkron ahyɛ me no enti, merekɔ Jerusalem a mennim asɛm a ɛbɛto me wɔ hɔ
2024-12-03 01:02:54,397 - functions - INFO - BLEU Score: 0.00
2024-12-03 01:02:54,397 - functions - INFO - --------------------------------------------------
2024-12-03 01:02:54,397 - functions - INFO - 
Aggregate Metrics:
2024-12-03 01:02:54,397 - functions - INFO - average_bleu: 0.00
2024-12-03 01:02:54,397 - functions - INFO - max_bleu: 0.00
2024-12-03 01:02:54,397 - functions - INFO - min_bleu: 0.00
2024-12-03 01:02:54,397 - functions - INFO - num_samples: 2
2024-12-03 01:02:54,398 - functions - INFO - Translation pipeline completed successfully
2024-12-03 01:02:54,398 - functions - INFO - === System Information ===
2024-12-03 01:02:54,398 - functions - INFO - PyTorch version: 2.5.0+cu124
2024-12-03 01:02:54,398 - functions - INFO - CUDA available: True
2024-12-03 01:02:54,398 - functions - INFO - Current device: 0
2024-12-03 01:02:54,398 - functions - INFO - Log file location: logs/mbart_TWI_ENGLISH_20241203_010254.log
2024-12-03 01:02:54,398 - functions - INFO - ==================================================
2024-12-03 01:02:54,398 - functions - INFO - Initializing mbart translator...
2024-12-03 01:02:54,398 - functions - INFO - === Translator Configuration ===
2024-12-03 01:02:54,398 - functions - INFO - Model name: facebook/mbart-large-50
2024-12-03 01:02:54,398 - functions - INFO - Max length: 128
2024-12-03 01:02:54,398 - functions - INFO - Batch size: 16
2024-12-03 01:02:54,398 - functions - INFO - Number of epochs: 1
2024-12-03 01:02:54,398 - functions - INFO - Learning rate: 1e-05
2024-12-03 01:02:54,398 - functions - INFO - Weight decay: 0.01
2024-12-03 01:02:54,398 - functions - INFO - Output directory: ./mbart_TWI_ENGLISH_20241203_010254
2024-12-03 01:02:54,398 - functions - INFO - Source lang: twi_GH
2024-12-03 01:02:54,398 - functions - INFO - Target lang: en_XX
2024-12-03 01:02:54,398 - functions - INFO - Source column: TWI
2024-12-03 01:02:54,398 - functions - INFO - Target column: ENGLISH
2024-12-03 01:02:54,398 - functions - INFO - ==================================================
2024-12-03 01:02:54,398 - functions - INFO - Using device: cuda
2024-12-03 01:02:58,835 - functions - INFO - Beginning model training procedures...
2024-12-03 01:03:08,410 - functions - INFO - Starting training...
2024-12-03 01:03:16,711 - functions - ERROR - Error computing metrics: 'score'
2024-12-03 01:03:27,709 - functions - INFO - Model saved to ./mbart_TWI_ENGLISH_20241203_010254
2024-12-03 01:03:27,710 - functions - INFO - Starting evaluation...
2024-12-03 01:03:29,386 - functions - ERROR - Error computing metrics: 'score'
2024-12-03 01:03:29,389 - functions - INFO - Training completed. Final metrics: {'eval_loss': 14.459030151367188, 'eval_bleu': 0.0, 'eval_runtime': 1.6739, 'eval_samples_per_second': 5.974, 'eval_steps_per_second': 0.597, 'epoch': 1.0}
2024-12-03 01:03:29,389 - functions - INFO - Beginning model evaluation...
2024-12-03 01:03:29,391 - functions - INFO - Generating translations for test sample...
2024-12-03 01:03:29,785 - functions - INFO - Translation results saved to ./mbart_TWI_ENGLISH_20241203_010254/sample_translations.csv
2024-12-03 01:03:29,786 - functions - INFO - Metrics saved to ./mbart_TWI_ENGLISH_20241203_010254/sample_metrics.csv
2024-12-03 01:03:29,786 - functions - INFO - 
Example Translations:
2024-12-03 01:03:29,786 - functions - INFO - --------------------------------------------------
2024-12-03 01:03:29,786 - functions - INFO - Source: Wɔrentumi mfa Ofir sikakɔkɔɔ ntɔ apopobibirieboɔ anaa aboɔdemmoɔ nso saa ara
2024-12-03 01:03:29,786 - functions - INFO - Reference: It is not valued with pure gold of Ophir, With precious onyx and sapphire
2024-12-03 01:03:29,786 - functions - INFO - Translation: W W W W W W W W W W W W W W W W W W
2024-12-03 01:03:29,786 - functions - INFO - BLEU Score: 0.00
2024-12-03 01:03:29,786 - functions - INFO - --------------------------------------------------
2024-12-03 01:03:29,786 - functions - INFO - Source: “Afei, ɛsiane sɛ Honhom Kronkron ahyɛ me no enti, merekɔ Yerusalem a mennim asɛm a ɛbɛto me wɔ hɔ
2024-12-03 01:03:29,786 - functions - INFO - Reference: And now, behold, I—bound in the Spirit—go on to Jerusalem, not knowing the things that will befall me in it
2024-12-03 01:03:29,786 - functions - INFO - Translation: “ “ “Afei, ɛsiane sɛ Honhom Kronkronkronkron ahyɛ me no enti, merekɔ Yerusalem a mennim asɛm a ɛbɛto me wɔ hɔ
2024-12-03 01:03:29,786 - functions - INFO - BLEU Score: 0.00
2024-12-03 01:03:29,786 - functions - INFO - --------------------------------------------------
2024-12-03 01:03:29,786 - functions - INFO - 
Aggregate Metrics:
2024-12-03 01:03:29,786 - functions - INFO - average_bleu: 0.00
2024-12-03 01:03:29,786 - functions - INFO - max_bleu: 0.00
2024-12-03 01:03:29,786 - functions - INFO - min_bleu: 0.00
2024-12-03 01:03:29,787 - functions - INFO - num_samples: 2
2024-12-03 01:03:29,787 - functions - INFO - Translation pipeline completed successfully
2024-12-03 01:03:29,787 - functions - INFO - === System Information ===
2024-12-03 01:03:29,787 - functions - INFO - PyTorch version: 2.5.0+cu124
2024-12-03 01:03:29,787 - functions - INFO - CUDA available: True
2024-12-03 01:03:29,787 - functions - INFO - Current device: 0
2024-12-03 01:03:29,787 - functions - INFO - Log file location: logs/mistral_TWI_ENGLISH_20241203_010329.log
2024-12-03 01:03:29,787 - functions - INFO - ==================================================
2024-12-03 01:03:29,787 - functions - INFO - Initializing mistral translator...
2024-12-03 01:03:36,011 - functions - INFO - === Mistral Translator Configuration ===
2024-12-03 01:03:36,011 - functions - INFO - Model name: mistralai/Mistral-7B-Instruct-v0.1
2024-12-03 01:03:36,011 - functions - INFO - Max length: 128
2024-12-03 01:03:36,011 - functions - INFO - Batch size: 3
2024-12-03 01:03:36,011 - functions - INFO - Number of epochs: 1
2024-12-03 01:03:36,011 - functions - INFO - Learning rate: 1e-05
2024-12-03 01:03:36,011 - functions - INFO - Weight decay: 0.01
2024-12-03 01:03:36,011 - functions - INFO - Source language: TWI
2024-12-03 01:03:36,011 - functions - INFO - Target language: ENGLISH
2024-12-03 01:03:36,011 - functions - INFO - Device: cuda
2024-12-03 01:03:36,015 - functions - INFO - Total parameters: 7,241,732,096
2024-12-03 01:03:36,015 - functions - INFO - Trainable parameters: 7,241,732,096
2024-12-03 01:03:36,015 - functions - INFO - ==================================================
2024-12-03 01:03:36,015 - functions - INFO - Beginning model training procedures...
2024-12-03 01:05:12,332 - functions - INFO - Starting training...
2024-12-03 01:05:16,083 - functions - ERROR - Error during training: CUDA out of memory. Tried to allocate 224.00 MiB. GPU 0 has a total capacity of 93.12 GiB of which 81.12 MiB is free. Including non-PyTorch memory, this process has 93.03 GiB memory in use. Of the allocated memory 91.49 GiB is allocated by PyTorch, and 115.24 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2024-12-03 01:05:16,083 - functions - ERROR - Error in mistral pipeline: CUDA out of memory. Tried to allocate 224.00 MiB. GPU 0 has a total capacity of 93.12 GiB of which 81.12 MiB is free. Including non-PyTorch memory, this process has 93.03 GiB memory in use. Of the allocated memory 91.49 GiB is allocated by PyTorch, and 115.24 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Traceback (most recent call last):
  File "/home/sheriff/research/twi/unified_model_trainer.py", line 45, in run_translation_pipeline
    metrics = translator.train(dataset)
              ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/sheriff/research/twi/functions.py", line 1610, in train
    trainer.train()
  File "/home/sheriff/miniconda3/envs/llm_loc/lib/python3.12/site-packages/transformers/trainer.py", line 2052, in train
    return inner_training_loop(
           ^^^^^^^^^^^^^^^^^^^^
  File "/home/sheriff/miniconda3/envs/llm_loc/lib/python3.12/site-packages/transformers/trainer.py", line 2452, in _inner_training_loop
    self.optimizer.step()
  File "/home/sheriff/miniconda3/envs/llm_loc/lib/python3.12/site-packages/torch/optim/lr_scheduler.py", line 137, in wrapper
    return func.__get__(opt, opt.__class__)(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/sheriff/miniconda3/envs/llm_loc/lib/python3.12/site-packages/torch/optim/optimizer.py", line 487, in wrapper
    out = func(*args, **kwargs)
          ^^^^^^^^^^^^^^^^^^^^^
  File "/home/sheriff/miniconda3/envs/llm_loc/lib/python3.12/site-packages/torch/optim/optimizer.py", line 91, in _use_grad
    ret = func(self, *args, **kwargs)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/sheriff/miniconda3/envs/llm_loc/lib/python3.12/site-packages/torch/optim/adamw.py", line 209, in step
    has_complex = self._init_group(
                  ^^^^^^^^^^^^^^^^^
  File "/home/sheriff/miniconda3/envs/llm_loc/lib/python3.12/site-packages/torch/optim/adamw.py", line 148, in _init_group
    state["exp_avg"] = torch.zeros_like(
                       ^^^^^^^^^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 224.00 MiB. GPU 0 has a total capacity of 93.12 GiB of which 81.12 MiB is free. Including non-PyTorch memory, this process has 93.03 GiB memory in use. Of the allocated memory 91.49 GiB is allocated by PyTorch, and 115.24 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2024-12-03 01:05:16,086 - functions - INFO - === System Information ===
2024-12-03 01:05:16,086 - functions - INFO - PyTorch version: 2.5.0+cu124
2024-12-03 01:05:16,086 - functions - INFO - CUDA available: True
2024-12-03 01:05:16,086 - functions - INFO - Current device: 0
2024-12-03 01:05:16,086 - functions - INFO - Log file location: logs/mt5_TWI_ENGLISH_20241203_010516.log
2024-12-03 01:05:16,086 - functions - INFO - ==================================================
2024-12-03 01:05:16,086 - functions - INFO - Initializing mt5 translator...
2024-12-03 01:05:20,050 - functions - INFO - Total parameters: 300176768
2024-12-03 01:05:20,050 - functions - INFO - Trainable parameters: 300176768
2024-12-03 01:05:20,054 - functions - INFO - Beginning model training procedures...
2024-12-03 01:05:20,054 - functions - INFO - Starting preprocessing...
2024-12-03 01:05:25,293 - functions - INFO - Processed train features: {'input_ids': Sequence(feature=Value(dtype='int32', id=None), length=-1, id=None), 'attention_mask': Sequence(feature=Value(dtype='int8', id=None), length=-1, id=None), 'labels': Sequence(feature=Value(dtype='int64', id=None), length=-1, id=None)}
2024-12-03 01:05:25,293 - functions - INFO - Sample processed input: {'input_ids': [89349, 259, 236855, 288, 68006, 138450, 267, 1995, 5349, 259, 4445, 7058, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'labels': [1385, 277, 263, 259, 4445, 7058, 260, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}
2024-12-03 01:05:25,418 - functions - INFO - Model device: cuda:0
2024-12-03 01:05:25,418 - functions - INFO - Starting training...
2024-12-03 01:05:39,295 - functions - ERROR - Error computing metrics: int() argument must be a string, a bytes-like object or a real number, not 'list'
2024-12-03 01:05:48,063 - functions - INFO - Model saved to ./mt5_TWI_ENGLISH_20241203_010516
2024-12-03 01:05:48,065 - functions - INFO - Training completed. Final metrics: None
2024-12-03 01:05:48,065 - functions - INFO - Beginning model evaluation...
2024-12-03 01:05:48,067 - functions - INFO - Generating translations for test sample...
2024-12-03 01:05:48,395 - functions - INFO - Translation results saved to ./mt5_TWI_ENGLISH_20241203_010516/sample_translations.csv
2024-12-03 01:05:48,395 - functions - INFO - Metrics saved to ./mt5_TWI_ENGLISH_20241203_010516/sample_metrics.csv
2024-12-03 01:05:48,395 - functions - INFO - 
Example Translations:
2024-12-03 01:05:48,395 - functions - INFO - --------------------------------------------------
2024-12-03 01:05:48,396 - functions - INFO - Source: Wɔrentumi mfa Ofir sikakɔkɔɔ ntɔ apopobibirieboɔ anaa aboɔdemmoɔ nso saa ara
2024-12-03 01:05:48,396 - functions - INFO - Reference: It is not valued with pure gold of Ophir, With precious onyx and sapphire
2024-12-03 01:05:48,396 - functions - INFO - Translation: <extra_id_0>
2024-12-03 01:05:48,396 - functions - INFO - BLEU Score: 0.00
2024-12-03 01:05:48,396 - functions - INFO - --------------------------------------------------
2024-12-03 01:05:48,396 - functions - INFO - Source: “Afei, ɛsiane sɛ Honhom Kronkron ahyɛ me no enti, merekɔ Yerusalem a mennim asɛm a ɛbɛto me wɔ hɔ
2024-12-03 01:05:48,396 - functions - INFO - Reference: And now, behold, I—bound in the Spirit—go on to Jerusalem, not knowing the things that will befall me in it
2024-12-03 01:05:48,396 - functions - INFO - Translation: <extra_id_0>
2024-12-03 01:05:48,396 - functions - INFO - BLEU Score: 0.00
2024-12-03 01:05:48,396 - functions - INFO - --------------------------------------------------
2024-12-03 01:05:48,396 - functions - INFO - 
Aggregate Metrics:
2024-12-03 01:05:48,396 - functions - INFO - average_bleu: 0.00
2024-12-03 01:05:48,396 - functions - INFO - max_bleu: 0.00
2024-12-03 01:05:48,396 - functions - INFO - min_bleu: 0.00
2024-12-03 01:05:48,396 - functions - INFO - num_samples: 2
2024-12-03 01:05:48,396 - functions - INFO - Translation pipeline completed successfully
2024-12-03 01:05:48,397 - functions - INFO - === System Information ===
2024-12-03 01:05:48,397 - functions - INFO - PyTorch version: 2.5.0+cu124
2024-12-03 01:05:48,397 - functions - INFO - CUDA available: True
2024-12-03 01:05:48,397 - functions - INFO - Current device: 0
2024-12-03 01:05:48,397 - functions - INFO - Log file location: logs/nllb_TWI_ENGLISH_20241203_010548.log
2024-12-03 01:05:48,397 - functions - INFO - ==================================================
2024-12-03 01:05:48,397 - functions - INFO - Initializing nllb translator...
2024-12-03 01:05:48,397 - functions - INFO - === NLLB Translator Configuration ===
2024-12-03 01:05:48,397 - functions - INFO - Model name: facebook/nllb-200-3.3B
2024-12-03 01:05:48,397 - functions - INFO - Max length: 128
2024-12-03 01:05:48,397 - functions - INFO - Batch size: 16
2024-12-03 01:05:48,397 - functions - INFO - Number of epochs: 1
2024-12-03 01:05:48,397 - functions - INFO - Learning rate: 1e-05
2024-12-03 01:05:48,397 - functions - INFO - Weight decay: 0.01
2024-12-03 01:05:48,397 - functions - INFO - Output directory: ./nllb_TWI_ENGLISH_20241203_010548
2024-12-03 01:05:48,397 - functions - INFO - Source language: twi_Latn
2024-12-03 01:05:48,397 - functions - INFO - Target language: eng_Latn
2024-12-03 01:05:48,397 - functions - INFO - Device: cuda
2024-12-03 01:05:48,397 - functions - INFO - ==================================================
2024-12-03 01:05:51,374 - functions - INFO - Total parameters: 3,344,863,232
2024-12-03 01:05:51,375 - functions - INFO - Trainable parameters: 3,344,863,232
2024-12-03 01:05:51,375 - functions - INFO - Using 4 GPUs
2024-12-03 01:05:51,375 - functions - INFO - Beginning model training procedures...
2024-12-03 01:06:33,275 - functions - INFO - Starting training...
2024-12-03 01:07:02,368 - functions - ERROR - Error computing metrics: argument 'ids': 'list' object cannot be interpreted as an integer
2024-12-03 01:07:37,096 - functions - INFO - Model saved to ./nllb_TWI_ENGLISH_20241203_010548
2024-12-03 01:07:37,099 - functions - INFO - Starting evaluation...
2024-12-03 01:07:45,418 - functions - ERROR - Error computing metrics: argument 'ids': 'list' object cannot be interpreted as an integer
2024-12-03 01:07:47,726 - functions - INFO - Training completed. Final metrics: {'eval_loss': 12.194456100463867, 'eval_bleu': 0.0, 'eval_runtime': 10.6209, 'eval_samples_per_second': 0.942, 'eval_steps_per_second': 0.094, 'epoch': 1.0}
2024-12-03 01:07:47,727 - functions - INFO - Beginning model evaluation...
2024-12-03 01:07:47,728 - functions - INFO - Generating translations for test sample...
2024-12-03 01:07:48,302 - functions - INFO - Translation results saved to ./nllb_TWI_ENGLISH_20241203_010548/sample_translations.csv
2024-12-03 01:07:48,303 - functions - INFO - Metrics saved to ./nllb_TWI_ENGLISH_20241203_010548/sample_metrics.csv
2024-12-03 01:07:48,303 - functions - INFO - 
Example Translations:
2024-12-03 01:07:48,303 - functions - INFO - --------------------------------------------------
2024-12-03 01:07:48,303 - functions - INFO - Source: Wɔrentumi mfa Ofir sikakɔkɔɔ ntɔ apopobibirieboɔ anaa aboɔdemmoɔ nso saa ara
2024-12-03 01:07:48,303 - functions - INFO - Reference: It is not valued with pure gold of Ophir, With precious onyx and sapphire
2024-12-03 01:07:48,303 - functions - INFO - Translation: The sapphire and the coral-stone are not purchased with gold of Ophir;
2024-12-03 01:07:48,303 - functions - INFO - BLEU Score: 0.00
2024-12-03 01:07:48,303 - functions - INFO - --------------------------------------------------
2024-12-03 01:07:48,303 - functions - INFO - Source: “Afei, ɛsiane sɛ Honhom Kronkron ahyɛ me no enti, merekɔ Yerusalem a mennim asɛm a ɛbɛto me wɔ hɔ
2024-12-03 01:07:48,303 - functions - INFO - Reference: And now, behold, I—bound in the Spirit—go on to Jerusalem, not knowing the things that will befall me in it
2024-12-03 01:07:48,303 - functions - INFO - Translation: "And now, compelled by the Spirit, I am going to Jerusalem, not knowing what will happen to me there.
2024-12-03 01:07:48,303 - functions - INFO - BLEU Score: 0.22
2024-12-03 01:07:48,303 - functions - INFO - --------------------------------------------------
2024-12-03 01:07:48,303 - functions - INFO - 
Aggregate Metrics:
2024-12-03 01:07:48,303 - functions - INFO - average_bleu: 0.11
2024-12-03 01:07:48,304 - functions - INFO - max_bleu: 0.22
2024-12-03 01:07:48,304 - functions - INFO - min_bleu: 0.00
2024-12-03 01:07:48,304 - functions - INFO - num_samples: 2
2024-12-03 01:07:48,304 - functions - INFO - Translation pipeline completed successfully
2024-12-03 01:07:48,304 - functions - INFO - === System Information ===
2024-12-03 01:07:48,304 - functions - INFO - PyTorch version: 2.5.0+cu124
2024-12-03 01:07:48,304 - functions - INFO - CUDA available: True
2024-12-03 01:07:48,304 - functions - INFO - Current device: 0
2024-12-03 01:07:48,304 - functions - INFO - Log file location: logs/opt_TWI_ENGLISH_20241203_010748.log
2024-12-03 01:07:48,304 - functions - INFO - ==================================================
2024-12-03 01:07:48,304 - functions - INFO - Initializing opt translator...
2024-12-03 01:07:57,621 - functions - INFO - === OPT Translator Configuration ===
2024-12-03 01:07:57,621 - functions - INFO - Model name: facebook/opt-350m
2024-12-03 01:07:57,621 - functions - INFO - Max length: 128
2024-12-03 01:07:57,621 - functions - INFO - Batch size: 16
2024-12-03 01:07:57,621 - functions - INFO - Number of epochs: 1
2024-12-03 01:07:57,622 - functions - INFO - Learning rate: 1e-05
2024-12-03 01:07:57,622 - functions - INFO - Weight decay: 0.01
2024-12-03 01:07:57,622 - functions - INFO - Source language: TWI
2024-12-03 01:07:57,622 - functions - INFO - Target language: ENGLISH
2024-12-03 01:07:57,622 - functions - INFO - Device: cuda
2024-12-03 01:07:57,624 - functions - INFO - Total parameters: 331,196,416
2024-12-03 01:07:57,624 - functions - INFO - Trainable parameters: 331,196,416
2024-12-03 01:07:57,624 - functions - INFO - ==================================================
2024-12-03 01:07:57,624 - functions - INFO - Beginning model training procedures...
2024-12-03 01:08:00,550 - functions - INFO - Starting training...
2024-12-03 01:08:06,027 - functions - INFO - BLEU score: 0.0000
2024-12-03 01:08:11,647 - functions - INFO - BLEU score: 0.0000
2024-12-03 01:08:18,720 - functions - INFO - BLEU score: 0.0000
2024-12-03 01:08:22,571 - functions - INFO - Model saved to ./opt_TWI_ENGLISH_20241203_010748
2024-12-03 01:08:23,504 - functions - INFO - BLEU score: 0.0000
2024-12-03 01:08:23,515 - functions - INFO - Training completed. Final metrics: {'eval_loss': 8.425249099731445, 'eval_bleu': 0.0, 'eval_runtime': 0.9311, 'eval_samples_per_second': 10.74, 'eval_steps_per_second': 2.148, 'epoch': 3.0}
2024-12-03 01:08:23,516 - functions - INFO - Beginning model evaluation...
2024-12-03 01:08:23,520 - functions - INFO - Generating translations for test sample...
2024-12-03 01:08:24,448 - functions - INFO - Translation results saved to ./opt_TWI_ENGLISH_20241203_010748/sample_translations.csv
2024-12-03 01:08:24,448 - functions - INFO - Metrics saved to ./opt_TWI_ENGLISH_20241203_010748/sample_metrics.csv
2024-12-03 01:08:24,448 - functions - INFO - 
Example Translations:
2024-12-03 01:08:24,448 - functions - INFO - --------------------------------------------------
2024-12-03 01:08:24,449 - functions - INFO - Source: Wɔrentumi mfa Ofir sikakɔkɔɔ ntɔ apopobibirieboɔ anaa aboɔdemmoɔ nso saa ara
2024-12-03 01:08:24,449 - functions - INFO - Reference: It is not valued with pure gold of Ophir, With precious onyx and sapphire
2024-12-03 01:08:24,449 - functions - INFO - Translation: >Wɔntumi Mfa ofir sikaɔ kɔbɔ bɔpɔ tɔnɔ pɔmɔ wɔrɔ rɔwɔ dɔ
2024-12-03 01:08:24,449 - functions - INFO - BLEU Score: 0.00
2024-12-03 01:08:24,449 - functions - INFO - --------------------------------------------------
2024-12-03 01:08:24,449 - functions - INFO - Source: “Afei, ɛsiane sɛ Honhom Kronkron ahyɛ me no enti, merekɔ Yerusalem a mennim asɛm a ɛbɛto me wɔ hɔ
2024-12-03 01:08:24,449 - functions - INFO - Reference: And now, behold, I—bound in the Spirit—go on to Jerusalem, not knowing the things that will befall me in it
2024-12-03 01:08:24,449 - functions - INFO - Translation: I’m not sure what you’re trying to say here, but it’s pretty clear that you don’t know how to speak English.
You're right, I didn't
2024-12-03 01:08:24,449 - functions - INFO - BLEU Score: 0.00
2024-12-03 01:08:24,449 - functions - INFO - --------------------------------------------------
2024-12-03 01:08:24,449 - functions - INFO - 
Aggregate Metrics:
2024-12-03 01:08:24,449 - functions - INFO - average_bleu: 0.00
2024-12-03 01:08:24,449 - functions - INFO - max_bleu: 0.00
2024-12-03 01:08:24,449 - functions - INFO - min_bleu: 0.00
2024-12-03 01:08:24,449 - functions - INFO - num_samples: 2
2024-12-03 01:08:24,450 - functions - INFO - Translation pipeline completed successfully
2024-12-03 01:08:24,450 - functions - INFO - === System Information ===
2024-12-03 01:08:24,450 - functions - INFO - PyTorch version: 2.5.0+cu124
2024-12-03 01:08:24,450 - functions - INFO - CUDA available: True
2024-12-03 01:08:24,450 - functions - INFO - Current device: 0
2024-12-03 01:08:24,450 - functions - INFO - Log file location: logs/xglm_TWI_ENGLISH_20241203_010824.log
2024-12-03 01:08:24,450 - functions - INFO - ==================================================
2024-12-03 01:08:24,450 - functions - INFO - Initializing xglm translator...
2024-12-03 01:08:29,968 - functions - INFO - === XGLM Translator Configuration ===
2024-12-03 01:08:29,969 - functions - INFO - Model name: facebook/xglm-564M
2024-12-03 01:08:29,969 - functions - INFO - Max length: 128
2024-12-03 01:08:29,969 - functions - INFO - Batch size: 16
2024-12-03 01:08:29,969 - functions - INFO - Number of epochs: 1
2024-12-03 01:08:29,969 - functions - INFO - Learning rate: 1e-05
2024-12-03 01:08:29,969 - functions - INFO - Weight decay: 0.01
2024-12-03 01:08:29,969 - functions - INFO - Source language: TWI
2024-12-03 01:08:29,969 - functions - INFO - Target language: ENGLISH
2024-12-03 01:08:29,969 - functions - INFO - Device: cuda
2024-12-03 01:08:29,970 - functions - INFO - Total parameters: 564,463,616
2024-12-03 01:08:29,970 - functions - INFO - Trainable parameters: 564,463,616
2024-12-03 01:08:29,970 - functions - INFO - ==================================================
2024-12-03 01:08:29,972 - functions - INFO - Beginning model training procedures...
2024-12-03 01:08:38,489 - functions - INFO - Starting training...
2024-12-03 01:08:47,863 - functions - INFO - BLEU score: 0.0000
2024-12-03 01:09:00,525 - functions - INFO - BLEU score: 0.0000
2024-12-03 01:09:14,871 - functions - INFO - BLEU score: 0.0000
2024-12-03 01:09:22,775 - functions - INFO - Model saved to ./xglm_TWI_ENGLISH_20241203_010824
2024-12-03 01:09:26,383 - functions - INFO - BLEU score: 0.0000
2024-12-03 01:09:26,718 - functions - INFO - Training completed. Final metrics: {'eval_loss': 11.270589828491211, 'eval_bleu': 0.0, 'eval_runtime': 3.6061, 'eval_samples_per_second': 2.773, 'eval_steps_per_second': 0.555, 'epoch': 3.0}
2024-12-03 01:09:26,718 - functions - INFO - Beginning model evaluation...
2024-12-03 01:09:26,720 - functions - INFO - Generating translations for test sample...
2024-12-03 01:09:27,651 - functions - INFO - Translation results saved to ./xglm_TWI_ENGLISH_20241203_010824/sample_translations.csv
2024-12-03 01:09:27,651 - functions - INFO - Metrics saved to ./xglm_TWI_ENGLISH_20241203_010824/sample_metrics.csv
2024-12-03 01:09:27,651 - functions - INFO - 
Example Translations:
2024-12-03 01:09:27,651 - functions - INFO - --------------------------------------------------
2024-12-03 01:09:27,651 - functions - INFO - Source: Wɔrentumi mfa Ofir sikakɔkɔɔ ntɔ apopobibirieboɔ anaa aboɔdemmoɔ nso saa ara
2024-12-03 01:09:27,651 - functions - INFO - Reference: It is not valued with pure gold of Ophir, With precious onyx and sapphire
2024-12-03 01:09:27,651 - functions - INFO - Translation: I'm sorry, but I don't understand what you're trying to say.
2024-12-03 01:09:27,651 - functions - INFO - BLEU Score: 0.00
2024-12-03 01:09:27,651 - functions - INFO - --------------------------------------------------
2024-12-03 01:09:27,652 - functions - INFO - Source: “Afei, ɛsiane sɛ Honhom Kronkron ahyɛ me no enti, merekɔ Yerusalem a mennim asɛm a ɛbɛto me wɔ hɔ
2024-12-03 01:09:27,652 - functions - INFO - Reference: And now, behold, I—bound in the Spirit—go on to Jerusalem, not knowing the things that will befall me in it
2024-12-03 01:09:27,652 - functions - INFO - Translation: I’m not sure what you mean by that, but it sounds like you’re talking about something else.”
2024-12-03 01:09:27,652 - functions - INFO - BLEU Score: 0.00
2024-12-03 01:09:27,652 - functions - INFO - --------------------------------------------------
2024-12-03 01:09:27,652 - functions - INFO - 
Aggregate Metrics:
2024-12-03 01:09:27,652 - functions - INFO - average_bleu: 0.00
2024-12-03 01:09:27,652 - functions - INFO - max_bleu: 0.00
2024-12-03 01:09:27,652 - functions - INFO - min_bleu: 0.00
2024-12-03 01:09:27,652 - functions - INFO - num_samples: 2
2024-12-03 01:09:27,652 - functions - INFO - Translation pipeline completed successfully
