2024-12-01 15:48:31,628 - functions - INFO - === System Information ===
2024-12-01 15:48:31,628 - functions - INFO - PyTorch version: 2.5.0+cu124
2024-12-01 15:48:32,309 - functions - INFO - CUDA available: True
2024-12-01 15:48:32,486 - functions - INFO - Current device: 0
2024-12-01 15:48:32,486 - functions - INFO - Log file location: logs/aya_TWI_ENGLISH_20241201_154831.log
2024-12-01 15:48:32,486 - functions - INFO - ==================================================
2024-12-01 15:48:32,486 - functions - INFO - Initializing aya translator...
2024-12-01 15:48:41,097 - functions - INFO - === Aya Translator Configuration ===
2024-12-01 15:48:41,097 - functions - INFO - Model name: CohereForAI/aya-23-8B
2024-12-01 15:48:41,098 - functions - INFO - Max length: 128
2024-12-01 15:48:41,098 - functions - INFO - Batch size: 16
2024-12-01 15:48:41,098 - functions - INFO - Number of epochs: 1
2024-12-01 15:48:41,098 - functions - INFO - Learning rate: 1e-05
2024-12-01 15:48:41,098 - functions - INFO - Weight decay: 0.01
2024-12-01 15:48:41,098 - functions - INFO - Source language: twi
2024-12-01 15:48:41,098 - functions - INFO - Target language: en
2024-12-01 15:48:41,098 - functions - INFO - Device: cuda
2024-12-01 15:48:41,101 - functions - INFO - Total parameters: 8,028,033,024
2024-12-01 15:48:41,101 - functions - INFO - Trainable parameters: 8,028,033,024
2024-12-01 15:48:41,101 - functions - INFO - ==================================================
2024-12-01 15:48:41,101 - functions - INFO - Beginning model training procedures...
2024-12-01 15:50:27,843 - functions - INFO - Starting training...
2024-12-01 15:50:36,125 - functions - ERROR - Error during training: CUDA out of memory. Tried to allocate 64.00 MiB. GPU 0 has a total capacity of 93.12 GiB of which 5.12 MiB is free. Including non-PyTorch memory, this process has 93.10 GiB memory in use. Of the allocated memory 88.22 GiB is allocated by PyTorch, and 4.00 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2024-12-01 15:50:36,125 - functions - ERROR - Error in aya pipeline: CUDA out of memory. Tried to allocate 64.00 MiB. GPU 0 has a total capacity of 93.12 GiB of which 5.12 MiB is free. Including non-PyTorch memory, this process has 93.10 GiB memory in use. Of the allocated memory 88.22 GiB is allocated by PyTorch, and 4.00 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Traceback (most recent call last):
  File "/home/sheriff/research/twi/unified_model_trainer.py", line 45, in run_translation_pipeline
    metrics = translator.train(dataset)
              ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/sheriff/research/twi/functions.py", line 1824, in train
    trainer.train()
  File "/home/sheriff/miniconda3/envs/llm_loc/lib/python3.12/site-packages/transformers/trainer.py", line 2052, in train
    return inner_training_loop(
           ^^^^^^^^^^^^^^^^^^^^
  File "/home/sheriff/miniconda3/envs/llm_loc/lib/python3.12/site-packages/transformers/trainer.py", line 2388, in _inner_training_loop
    tr_loss_step = self.training_step(model, inputs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/sheriff/miniconda3/envs/llm_loc/lib/python3.12/site-packages/transformers/trainer.py", line 3518, in training_step
    self.accelerator.backward(loss, **kwargs)
  File "/home/sheriff/miniconda3/envs/llm_loc/lib/python3.12/site-packages/accelerate/accelerator.py", line 2242, in backward
    self.scaler.scale(loss).backward(**kwargs)
  File "/home/sheriff/miniconda3/envs/llm_loc/lib/python3.12/site-packages/torch/_tensor.py", line 581, in backward
    torch.autograd.backward(
  File "/home/sheriff/miniconda3/envs/llm_loc/lib/python3.12/site-packages/torch/autograd/__init__.py", line 347, in backward
    _engine_run_backward(
  File "/home/sheriff/miniconda3/envs/llm_loc/lib/python3.12/site-packages/torch/autograd/graph.py", line 825, in _engine_run_backward
    return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/sheriff/miniconda3/envs/llm_loc/lib/python3.12/site-packages/torch/autograd/function.py", line 307, in apply
    return user_fn(self, *args)
           ^^^^^^^^^^^^^^^^^^^^
  File "/home/sheriff/miniconda3/envs/llm_loc/lib/python3.12/site-packages/torch/nn/parallel/_functions.py", line 33, in backward
    return (None,) + ReduceAddCoalesced.apply(
                     ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/sheriff/miniconda3/envs/llm_loc/lib/python3.12/site-packages/torch/autograd/function.py", line 575, in apply
    return super().apply(*args, **kwargs)  # type: ignore[misc]
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/sheriff/miniconda3/envs/llm_loc/lib/python3.12/site-packages/torch/nn/parallel/_functions.py", line 46, in forward
    return comm.reduce_add_coalesced(grads_, destination)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/sheriff/miniconda3/envs/llm_loc/lib/python3.12/site-packages/torch/nn/parallel/comm.py", line 159, in reduce_add_coalesced
    flat_result = reduce_add(flat_tensors, destination)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/sheriff/miniconda3/envs/llm_loc/lib/python3.12/site-packages/torch/nn/parallel/comm.py", line 107, in reduce_add
    result = torch.empty_like(inputs[root_index])
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 64.00 MiB. GPU 0 has a total capacity of 93.12 GiB of which 5.12 MiB is free. Including non-PyTorch memory, this process has 93.10 GiB memory in use. Of the allocated memory 88.22 GiB is allocated by PyTorch, and 4.00 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2024-12-01 15:50:36,134 - functions - INFO - === System Information ===
2024-12-01 15:50:36,134 - functions - INFO - PyTorch version: 2.5.0+cu124
2024-12-01 15:50:36,134 - functions - INFO - CUDA available: True
2024-12-01 15:50:36,134 - functions - INFO - Current device: 0
2024-12-01 15:50:36,134 - functions - INFO - Log file location: logs/falcon_TWI_ENGLISH_20241201_155036.log
2024-12-01 15:50:36,134 - functions - INFO - ==================================================
2024-12-01 15:50:36,134 - functions - INFO - Initializing falcon translator...
2024-12-01 15:50:37,772 - transformers_modules.tiiuae.falcon-7b.ec89142b67d748a1865ea4451372db8313ada0d8.configuration_falcon - WARNING - 
WARNING: You are currently loading Falcon using legacy code contained in the model repository. Falcon has now been fully ported into the Hugging Face transformers library. For the most up-to-date and high-performance version of the Falcon model code, please update to the latest version of transformers and then load the model without the trust_remote_code=True argument.

2024-12-01 15:52:58,444 - functions - INFO - === Falcon Translator Configuration ===
2024-12-01 15:52:58,444 - functions - INFO - Model name: tiiuae/falcon-7b
2024-12-01 15:52:58,444 - functions - INFO - Max length: 128
2024-12-01 15:52:58,444 - functions - INFO - Batch size: 16
2024-12-01 15:52:58,444 - functions - INFO - Number of epochs: 1
2024-12-01 15:52:58,444 - functions - INFO - Learning rate: 1e-05
2024-12-01 15:52:58,444 - functions - INFO - Weight decay: 0.01
2024-12-01 15:52:58,444 - functions - INFO - Source language: TWI
2024-12-01 15:52:58,444 - functions - INFO - Target language: ENGLISH
2024-12-01 15:52:58,444 - functions - INFO - Device: cuda
2024-12-01 15:52:58,445 - functions - INFO - Total parameters: 6,921,720,704
2024-12-01 15:52:58,445 - functions - INFO - Trainable parameters: 6,921,720,704
2024-12-01 15:52:58,445 - functions - INFO - ==================================================
2024-12-01 15:52:58,445 - functions - INFO - Beginning model training procedures...
2024-12-01 15:53:44,402 - functions - ERROR - Error in preprocessing: Asking to pad but the tokenizer does not have a padding token. Please select a token to use as `pad_token` `(tokenizer.pad_token = tokenizer.eos_token e.g.)` or add a new pad token via `tokenizer.add_special_tokens({'pad_token': '[PAD]'})`.
2024-12-01 15:53:44,402 - functions - ERROR - Error during training: Asking to pad but the tokenizer does not have a padding token. Please select a token to use as `pad_token` `(tokenizer.pad_token = tokenizer.eos_token e.g.)` or add a new pad token via `tokenizer.add_special_tokens({'pad_token': '[PAD]'})`.
2024-12-01 15:53:44,402 - functions - ERROR - Error in falcon pipeline: Asking to pad but the tokenizer does not have a padding token. Please select a token to use as `pad_token` `(tokenizer.pad_token = tokenizer.eos_token e.g.)` or add a new pad token via `tokenizer.add_special_tokens({'pad_token': '[PAD]'})`.
Traceback (most recent call last):
  File "/home/sheriff/research/twi/unified_model_trainer.py", line 45, in run_translation_pipeline
    metrics = translator.train(dataset)
              ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/sheriff/research/twi/functions.py", line 2205, in train
    processed_datasets = dataset.map(
                         ^^^^^^^^^^^^
  File "/home/sheriff/miniconda3/envs/llm_loc/lib/python3.12/site-packages/datasets/dataset_dict.py", line 867, in map
    k: dataset.map(
       ^^^^^^^^^^^^
  File "/home/sheriff/miniconda3/envs/llm_loc/lib/python3.12/site-packages/datasets/arrow_dataset.py", line 560, in wrapper
    out: Union["Dataset", "DatasetDict"] = func(self, *args, **kwargs)
                                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/sheriff/miniconda3/envs/llm_loc/lib/python3.12/site-packages/datasets/arrow_dataset.py", line 3035, in map
    for rank, done, content in Dataset._map_single(**dataset_kwargs):
  File "/home/sheriff/miniconda3/envs/llm_loc/lib/python3.12/site-packages/datasets/arrow_dataset.py", line 3438, in _map_single
    batch = apply_function_on_filtered_inputs(
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/sheriff/miniconda3/envs/llm_loc/lib/python3.12/site-packages/datasets/arrow_dataset.py", line 3300, in apply_function_on_filtered_inputs
    processed_inputs = function(*fn_args, *additional_args, **fn_kwargs)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/sheriff/research/twi/functions.py", line 2152, in preprocess_function
    model_inputs = self.tokenizer(
                   ^^^^^^^^^^^^^^^
  File "/home/sheriff/miniconda3/envs/llm_loc/lib/python3.12/site-packages/transformers/tokenization_utils_base.py", line 3016, in __call__
    encodings = self._call_one(text=text, text_pair=text_pair, **all_kwargs)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/sheriff/miniconda3/envs/llm_loc/lib/python3.12/site-packages/transformers/tokenization_utils_base.py", line 3104, in _call_one
    return self.batch_encode_plus(
           ^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/sheriff/miniconda3/envs/llm_loc/lib/python3.12/site-packages/transformers/tokenization_utils_base.py", line 3297, in batch_encode_plus
    padding_strategy, truncation_strategy, max_length, kwargs = self._get_padding_truncation_strategies(
                                                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/sheriff/miniconda3/envs/llm_loc/lib/python3.12/site-packages/transformers/tokenization_utils_base.py", line 2918, in _get_padding_truncation_strategies
    raise ValueError(
ValueError: Asking to pad but the tokenizer does not have a padding token. Please select a token to use as `pad_token` `(tokenizer.pad_token = tokenizer.eos_token e.g.)` or add a new pad token via `tokenizer.add_special_tokens({'pad_token': '[PAD]'})`.
2024-12-01 15:53:44,405 - functions - INFO - === System Information ===
2024-12-01 15:53:44,405 - functions - INFO - PyTorch version: 2.5.0+cu124
2024-12-01 15:53:44,405 - functions - INFO - CUDA available: True
2024-12-01 15:53:44,405 - functions - INFO - Current device: 0
2024-12-01 15:53:44,405 - functions - INFO - Log file location: logs/llama_TWI_ENGLISH_20241201_155344.log
2024-12-01 15:53:44,405 - functions - INFO - ==================================================
2024-12-01 15:53:44,405 - functions - INFO - Initializing llama translator...
2024-12-01 15:53:46,938 - functions - INFO - === Llama Translator Configuration ===
2024-12-01 15:53:46,938 - functions - INFO - Model name: meta-llama/Llama-3.2-1B
2024-12-01 15:53:46,938 - functions - INFO - Max length: 128
2024-12-01 15:53:46,938 - functions - INFO - Batch size: 16
2024-12-01 15:53:46,938 - functions - INFO - Number of epochs: 1
2024-12-01 15:53:46,938 - functions - INFO - Learning rate: 1e-05
2024-12-01 15:53:46,938 - functions - INFO - Weight decay: 0.01
2024-12-01 15:53:46,938 - functions - INFO - Source language: TWI
2024-12-01 15:53:46,938 - functions - INFO - Target language: ENGLISH
2024-12-01 15:53:46,938 - functions - INFO - Device: cuda
2024-12-01 15:53:46,940 - functions - INFO - Total parameters: 1,235,814,400
2024-12-01 15:53:46,940 - functions - INFO - Trainable parameters: 1,235,814,400
2024-12-01 15:53:46,940 - functions - INFO - ==================================================
2024-12-01 15:53:46,967 - functions - INFO - Beginning model training procedures...
2024-12-01 15:54:04,394 - functions - INFO - Starting training...
2024-12-01 15:54:16,696 - functions - ERROR - Error computing metrics: argument 'ids': 'list' object cannot be interpreted as an integer
2024-12-01 15:54:42,045 - functions - ERROR - Error computing metrics: argument 'ids': 'list' object cannot be interpreted as an integer
2024-12-01 15:55:20,948 - functions - ERROR - Error computing metrics: argument 'ids': 'list' object cannot be interpreted as an integer
2024-12-01 15:55:50,050 - functions - INFO - Model saved to ./llama_TWI_ENGLISH_20241201_155344
2024-12-01 15:55:52,970 - functions - ERROR - Error computing metrics: argument 'ids': 'list' object cannot be interpreted as an integer
2024-12-01 15:55:53,577 - functions - INFO - Training completed. Final metrics: {'eval_loss': 7.220907688140869, 'eval_bleu': 0.0, 'eval_runtime': 3.3422, 'eval_samples_per_second': 2.992, 'eval_steps_per_second': 0.598, 'epoch': 3.0}
2024-12-01 15:55:53,577 - functions - INFO - Beginning model evaluation...
2024-12-01 15:55:53,579 - functions - INFO - Generating translations for test sample...
2024-12-01 15:55:53,580 - functions - ERROR - Error during translation: name 'text' is not defined
2024-12-01 15:55:53,580 - functions - ERROR - Error during sample translation: name 'text' is not defined
Traceback (most recent call last):
  File "/home/sheriff/research/twi/functions.py", line 101, in evaluate_model
    translated = translator.translate(src_text)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/sheriff/research/twi/functions.py", line 1201, in translate
    text,
    ^^^^
NameError: name 'text' is not defined. Did you mean: 'next'?
2024-12-01 15:55:53,580 - functions - ERROR - Error in llama pipeline: name 'text' is not defined
Traceback (most recent call last):
  File "/home/sheriff/research/twi/unified_model_trainer.py", line 50, in run_translation_pipeline
    evaluate_model(
  File "/home/sheriff/research/twi/functions.py", line 101, in evaluate_model
    translated = translator.translate(src_text)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/sheriff/research/twi/functions.py", line 1201, in translate
    text,
    ^^^^
NameError: name 'text' is not defined. Did you mean: 'next'?
2024-12-01 15:55:53,581 - functions - INFO - === System Information ===
2024-12-01 15:55:53,581 - functions - INFO - PyTorch version: 2.5.0+cu124
2024-12-01 15:55:53,581 - functions - INFO - CUDA available: True
2024-12-01 15:55:53,581 - functions - INFO - Current device: 0
2024-12-01 15:55:53,581 - functions - INFO - Log file location: logs/m2m_TWI_ENGLISH_20241201_155553.log
2024-12-01 15:55:53,581 - functions - INFO - ==================================================
2024-12-01 15:55:53,581 - functions - INFO - Initializing m2m translator...
2024-12-01 15:55:55,696 - functions - INFO - === M2M Translator Configuration ===
2024-12-01 15:55:55,696 - functions - INFO - Model name: facebook/m2m100_1.2B
2024-12-01 15:55:55,696 - functions - INFO - Max length: 128
2024-12-01 15:55:55,696 - functions - INFO - Batch size: 16
2024-12-01 15:55:55,696 - functions - INFO - Number of epochs: 1
2024-12-01 15:55:55,696 - functions - INFO - Learning rate: 1e-05
2024-12-01 15:55:55,696 - functions - INFO - Weight decay: 0.01
2024-12-01 15:55:55,696 - functions - INFO - Source language: fr
2024-12-01 15:55:55,696 - functions - INFO - Target language: en
2024-12-01 15:55:55,696 - functions - INFO - Device: cuda
2024-12-01 15:55:55,698 - functions - INFO - Total parameters: 1,239,470,080
2024-12-01 15:55:55,699 - functions - INFO - Trainable parameters: 1,239,470,080
2024-12-01 15:55:55,699 - functions - INFO - ==================================================
2024-12-01 15:55:55,699 - functions - INFO - Beginning model training procedures...
2024-12-01 15:56:15,560 - functions - INFO - Starting training...
2024-12-01 15:56:32,480 - functions - ERROR - Error computing metrics: 'score'
2024-12-01 15:57:03,906 - functions - INFO - Model saved to ./m2m_TWI_ENGLISH_20241201_155553
2024-12-01 15:57:05,730 - functions - ERROR - Error computing metrics: 'score'
2024-12-01 15:57:05,734 - functions - INFO - Training completed. Final metrics: {'eval_loss': 10.178234100341797, 'eval_bleu': 0.0, 'eval_runtime': 1.8231, 'eval_samples_per_second': 5.485, 'eval_steps_per_second': 0.549, 'epoch': 1.0}
2024-12-01 15:57:05,734 - functions - INFO - Beginning model evaluation...
2024-12-01 15:57:05,736 - functions - INFO - Generating translations for test sample...
2024-12-01 15:57:06,661 - functions - INFO - Translation results saved to ./m2m_TWI_ENGLISH_20241201_155553/sample_translations.csv
2024-12-01 15:57:06,662 - functions - INFO - Metrics saved to ./m2m_TWI_ENGLISH_20241201_155553/sample_metrics.csv
2024-12-01 15:57:06,662 - functions - INFO - 
Example Translations:
2024-12-01 15:57:06,662 - functions - INFO - --------------------------------------------------
2024-12-01 15:57:06,662 - functions - INFO - Source: Wɔrentumi mfa Ofir sikakɔkɔɔ ntɔ apopobibirieboɔ anaa aboɔdemmoɔ nso saa ara
2024-12-01 15:57:06,662 - functions - INFO - Reference: It is not valued with pure gold of Ophir, With precious onyx and sapphire
2024-12-01 15:57:06,662 - functions - INFO - Translation: Wɔrentumi mfa Ofir sikakɔŋɔ ntɔ apopobibirieboɔ anaa aboɔdemmoɔ nso saa ara
2024-12-01 15:57:06,662 - functions - INFO - BLEU Score: 0.00
2024-12-01 15:57:06,662 - functions - INFO - --------------------------------------------------
2024-12-01 15:57:06,662 - functions - INFO - Source: “Afei, ɛsiane sɛ Honhom Kronkron ahyɛ me no enti, merekɔ Yerusalem a mennim asɛm a ɛbɛto me wɔ hɔ
2024-12-01 15:57:06,662 - functions - INFO - Reference: And now, behold, I—bound in the Spirit—go on to Jerusalem, not knowing the things that will befall me in it
2024-12-01 15:57:06,662 - functions - INFO - Translation: “Afei, ɛsiane sɛ Honhom Kronkron ahyɛ me no enti, merekɔ Jerusalem a mennim asɛm a ɛbɛto me wɔ hɔ
2024-12-01 15:57:06,662 - functions - INFO - BLEU Score: 0.00
2024-12-01 15:57:06,662 - functions - INFO - --------------------------------------------------
2024-12-01 15:57:06,662 - functions - INFO - 
Aggregate Metrics:
2024-12-01 15:57:06,663 - functions - INFO - average_bleu: 0.00
2024-12-01 15:57:06,663 - functions - INFO - max_bleu: 0.00
2024-12-01 15:57:06,663 - functions - INFO - min_bleu: 0.00
2024-12-01 15:57:06,663 - functions - INFO - num_samples: 2
2024-12-01 15:57:06,663 - functions - INFO - Translation pipeline completed successfully
2024-12-01 15:57:06,663 - functions - INFO - === System Information ===
2024-12-01 15:57:06,663 - functions - INFO - PyTorch version: 2.5.0+cu124
2024-12-01 15:57:06,663 - functions - INFO - CUDA available: True
2024-12-01 15:57:06,663 - functions - INFO - Current device: 0
2024-12-01 15:57:06,663 - functions - INFO - Log file location: logs/mbart_TWI_ENGLISH_20241201_155706.log
2024-12-01 15:57:06,663 - functions - INFO - ==================================================
2024-12-01 15:57:06,663 - functions - INFO - Initializing mbart translator...
2024-12-01 15:57:06,663 - functions - INFO - === Translator Configuration ===
2024-12-01 15:57:06,663 - functions - INFO - Model name: facebook/mbart-large-50
2024-12-01 15:57:06,663 - functions - INFO - Max length: 128
2024-12-01 15:57:06,663 - functions - INFO - Batch size: 16
2024-12-01 15:57:06,663 - functions - INFO - Number of epochs: 1
2024-12-01 15:57:06,664 - functions - INFO - Learning rate: 1e-05
2024-12-01 15:57:06,664 - functions - INFO - Weight decay: 0.01
2024-12-01 15:57:06,664 - functions - INFO - Output directory: ./mbart_TWI_ENGLISH_20241201_155706
2024-12-01 15:57:06,664 - functions - INFO - Source lang: twi_GH
2024-12-01 15:57:06,664 - functions - INFO - Target lang: en_XX
2024-12-01 15:57:06,664 - functions - INFO - Source column: TWI
2024-12-01 15:57:06,664 - functions - INFO - Target column: ENGLISH
2024-12-01 15:57:06,664 - functions - INFO - ==================================================
2024-12-01 15:57:06,664 - functions - INFO - Using device: cuda
2024-12-01 15:57:11,127 - functions - INFO - Beginning model training procedures...
2024-12-01 15:57:20,392 - functions - INFO - Starting training...
2024-12-01 15:57:28,774 - functions - ERROR - Error computing metrics: 'score'
2024-12-01 15:57:38,762 - functions - INFO - Model saved to ./mbart_TWI_ENGLISH_20241201_155706
2024-12-01 15:57:38,763 - functions - INFO - Starting evaluation...
2024-12-01 15:57:40,438 - functions - ERROR - Error computing metrics: 'score'
2024-12-01 15:57:40,440 - functions - INFO - Training completed. Final metrics: {'eval_loss': 14.459030151367188, 'eval_bleu': 0.0, 'eval_runtime': 1.6737, 'eval_samples_per_second': 5.975, 'eval_steps_per_second': 0.597, 'epoch': 1.0}
2024-12-01 15:57:40,440 - functions - INFO - Beginning model evaluation...
2024-12-01 15:57:40,441 - functions - INFO - Generating translations for test sample...
2024-12-01 15:57:40,831 - functions - INFO - Translation results saved to ./mbart_TWI_ENGLISH_20241201_155706/sample_translations.csv
2024-12-01 15:57:40,832 - functions - INFO - Metrics saved to ./mbart_TWI_ENGLISH_20241201_155706/sample_metrics.csv
2024-12-01 15:57:40,832 - functions - INFO - 
Example Translations:
2024-12-01 15:57:40,832 - functions - INFO - --------------------------------------------------
2024-12-01 15:57:40,832 - functions - INFO - Source: Wɔrentumi mfa Ofir sikakɔkɔɔ ntɔ apopobibirieboɔ anaa aboɔdemmoɔ nso saa ara
2024-12-01 15:57:40,832 - functions - INFO - Reference: It is not valued with pure gold of Ophir, With precious onyx and sapphire
2024-12-01 15:57:40,832 - functions - INFO - Translation: W W W W W W W W W W W W W W W W W W
2024-12-01 15:57:40,832 - functions - INFO - BLEU Score: 0.00
2024-12-01 15:57:40,832 - functions - INFO - --------------------------------------------------
2024-12-01 15:57:40,832 - functions - INFO - Source: “Afei, ɛsiane sɛ Honhom Kronkron ahyɛ me no enti, merekɔ Yerusalem a mennim asɛm a ɛbɛto me wɔ hɔ
2024-12-01 15:57:40,832 - functions - INFO - Reference: And now, behold, I—bound in the Spirit—go on to Jerusalem, not knowing the things that will befall me in it
2024-12-01 15:57:40,832 - functions - INFO - Translation: “ “ “Afei, ɛsiane sɛ Honhom Kronkronkronkron ahyɛ me no enti, merekɔ Yerusalem a mennim asɛm a ɛbɛto me wɔ hɔ
2024-12-01 15:57:40,833 - functions - INFO - BLEU Score: 0.00
2024-12-01 15:57:40,833 - functions - INFO - --------------------------------------------------
2024-12-01 15:57:40,833 - functions - INFO - 
Aggregate Metrics:
2024-12-01 15:57:40,833 - functions - INFO - average_bleu: 0.00
2024-12-01 15:57:40,833 - functions - INFO - max_bleu: 0.00
2024-12-01 15:57:40,833 - functions - INFO - min_bleu: 0.00
2024-12-01 15:57:40,833 - functions - INFO - num_samples: 2
2024-12-01 15:57:40,833 - functions - INFO - Translation pipeline completed successfully
2024-12-01 15:57:40,833 - functions - INFO - === System Information ===
2024-12-01 15:57:40,833 - functions - INFO - PyTorch version: 2.5.0+cu124
2024-12-01 15:57:40,833 - functions - INFO - CUDA available: True
2024-12-01 15:57:40,833 - functions - INFO - Current device: 0
2024-12-01 15:57:40,833 - functions - INFO - Log file location: logs/mistral_TWI_ENGLISH_20241201_155740.log
2024-12-01 15:57:40,833 - functions - INFO - ==================================================
2024-12-01 15:57:40,833 - functions - INFO - Initializing mistral translator...
2024-12-01 15:59:55,778 - functions - INFO - === Mistral Translator Configuration ===
2024-12-01 15:59:55,779 - functions - INFO - Model name: mistralai/Mistral-7B-Instruct-v0.1
2024-12-01 15:59:55,779 - functions - INFO - Max length: 128
2024-12-01 15:59:55,779 - functions - INFO - Batch size: 3
2024-12-01 15:59:55,779 - functions - INFO - Number of epochs: 1
2024-12-01 15:59:55,779 - functions - INFO - Learning rate: 1e-05
2024-12-01 15:59:55,779 - functions - INFO - Weight decay: 0.01
2024-12-01 15:59:55,779 - functions - INFO - Source language: twi
2024-12-01 15:59:55,779 - functions - INFO - Target language: eng
2024-12-01 15:59:55,779 - functions - INFO - Device: cuda
2024-12-01 15:59:55,782 - functions - INFO - Total parameters: 7,241,732,096
2024-12-01 15:59:55,782 - functions - INFO - Trainable parameters: 7,241,732,096
2024-12-01 15:59:55,782 - functions - INFO - ==================================================
2024-12-01 15:59:55,782 - functions - INFO - Beginning model training procedures...
2024-12-01 16:00:44,023 - functions - ERROR - Error in preprocessing: Asking to pad but the tokenizer does not have a padding token. Please select a token to use as `pad_token` `(tokenizer.pad_token = tokenizer.eos_token e.g.)` or add a new pad token via `tokenizer.add_special_tokens({'pad_token': '[PAD]'})`.
2024-12-01 16:00:44,023 - functions - ERROR - Error during training: Asking to pad but the tokenizer does not have a padding token. Please select a token to use as `pad_token` `(tokenizer.pad_token = tokenizer.eos_token e.g.)` or add a new pad token via `tokenizer.add_special_tokens({'pad_token': '[PAD]'})`.
2024-12-01 16:00:44,023 - functions - ERROR - Error in mistral pipeline: Asking to pad but the tokenizer does not have a padding token. Please select a token to use as `pad_token` `(tokenizer.pad_token = tokenizer.eos_token e.g.)` or add a new pad token via `tokenizer.add_special_tokens({'pad_token': '[PAD]'})`.
Traceback (most recent call last):
  File "/home/sheriff/research/twi/unified_model_trainer.py", line 45, in run_translation_pipeline
    metrics = translator.train(dataset)
              ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/sheriff/research/twi/functions.py", line 1566, in train
    processed_datasets = dataset.map(
                         ^^^^^^^^^^^^
  File "/home/sheriff/miniconda3/envs/llm_loc/lib/python3.12/site-packages/datasets/dataset_dict.py", line 867, in map
    k: dataset.map(
       ^^^^^^^^^^^^
  File "/home/sheriff/miniconda3/envs/llm_loc/lib/python3.12/site-packages/datasets/arrow_dataset.py", line 560, in wrapper
    out: Union["Dataset", "DatasetDict"] = func(self, *args, **kwargs)
                                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/sheriff/miniconda3/envs/llm_loc/lib/python3.12/site-packages/datasets/arrow_dataset.py", line 3035, in map
    for rank, done, content in Dataset._map_single(**dataset_kwargs):
  File "/home/sheriff/miniconda3/envs/llm_loc/lib/python3.12/site-packages/datasets/arrow_dataset.py", line 3438, in _map_single
    batch = apply_function_on_filtered_inputs(
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/sheriff/miniconda3/envs/llm_loc/lib/python3.12/site-packages/datasets/arrow_dataset.py", line 3300, in apply_function_on_filtered_inputs
    processed_inputs = function(*fn_args, *additional_args, **fn_kwargs)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/sheriff/research/twi/functions.py", line 1513, in preprocess_function
    model_inputs = self.tokenizer(
                   ^^^^^^^^^^^^^^^
  File "/home/sheriff/miniconda3/envs/llm_loc/lib/python3.12/site-packages/transformers/tokenization_utils_base.py", line 3016, in __call__
    encodings = self._call_one(text=text, text_pair=text_pair, **all_kwargs)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/sheriff/miniconda3/envs/llm_loc/lib/python3.12/site-packages/transformers/tokenization_utils_base.py", line 3104, in _call_one
    return self.batch_encode_plus(
           ^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/sheriff/miniconda3/envs/llm_loc/lib/python3.12/site-packages/transformers/tokenization_utils_base.py", line 3297, in batch_encode_plus
    padding_strategy, truncation_strategy, max_length, kwargs = self._get_padding_truncation_strategies(
                                                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/sheriff/miniconda3/envs/llm_loc/lib/python3.12/site-packages/transformers/tokenization_utils_base.py", line 2918, in _get_padding_truncation_strategies
    raise ValueError(
ValueError: Asking to pad but the tokenizer does not have a padding token. Please select a token to use as `pad_token` `(tokenizer.pad_token = tokenizer.eos_token e.g.)` or add a new pad token via `tokenizer.add_special_tokens({'pad_token': '[PAD]'})`.
2024-12-01 16:00:44,025 - functions - INFO - === System Information ===
2024-12-01 16:00:44,025 - functions - INFO - PyTorch version: 2.5.0+cu124
2024-12-01 16:00:44,025 - functions - INFO - CUDA available: True
2024-12-01 16:00:44,025 - functions - INFO - Current device: 0
2024-12-01 16:00:44,025 - functions - INFO - Log file location: logs/mt5_TWI_ENGLISH_20241201_160044.log
2024-12-01 16:00:44,025 - functions - INFO - ==================================================
2024-12-01 16:00:44,025 - functions - INFO - Initializing mt5 translator...
2024-12-01 16:00:47,858 - functions - INFO - Total parameters: 300176768
2024-12-01 16:00:47,859 - functions - INFO - Trainable parameters: 300176768
2024-12-01 16:00:47,873 - functions - INFO - Beginning model training procedures...
2024-12-01 16:00:47,873 - functions - INFO - Starting preprocessing...
2024-12-01 16:00:53,106 - functions - INFO - Processed train features: {'input_ids': Sequence(feature=Value(dtype='int32', id=None), length=-1, id=None), 'attention_mask': Sequence(feature=Value(dtype='int8', id=None), length=-1, id=None), 'labels': Sequence(feature=Value(dtype='int64', id=None), length=-1, id=None)}
2024-12-01 16:00:53,106 - functions - INFO - Sample processed input: {'input_ids': [89349, 259, 236855, 288, 68006, 138450, 267, 1995, 5349, 259, 4445, 7058, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'labels': [1385, 277, 263, 259, 4445, 7058, 260, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}
2024-12-01 16:00:53,217 - functions - INFO - Model device: cuda:0
2024-12-01 16:00:53,217 - functions - INFO - Starting training...
2024-12-01 16:01:06,006 - functions - ERROR - Error computing metrics: int() argument must be a string, a bytes-like object or a real number, not 'list'
2024-12-01 16:01:16,237 - functions - INFO - Model saved to ./mt5_TWI_ENGLISH_20241201_160044
2024-12-01 16:01:16,238 - functions - INFO - Training completed. Final metrics: None
2024-12-01 16:01:16,238 - functions - INFO - Beginning model evaluation...
2024-12-01 16:01:16,241 - functions - INFO - Generating translations for test sample...
2024-12-01 16:01:16,570 - functions - INFO - Translation results saved to ./mt5_TWI_ENGLISH_20241201_160044/sample_translations.csv
2024-12-01 16:01:16,570 - functions - INFO - Metrics saved to ./mt5_TWI_ENGLISH_20241201_160044/sample_metrics.csv
2024-12-01 16:01:16,571 - functions - INFO - 
Example Translations:
2024-12-01 16:01:16,571 - functions - INFO - --------------------------------------------------
2024-12-01 16:01:16,571 - functions - INFO - Source: Wɔrentumi mfa Ofir sikakɔkɔɔ ntɔ apopobibirieboɔ anaa aboɔdemmoɔ nso saa ara
2024-12-01 16:01:16,571 - functions - INFO - Reference: It is not valued with pure gold of Ophir, With precious onyx and sapphire
2024-12-01 16:01:16,571 - functions - INFO - Translation: <extra_id_0>
2024-12-01 16:01:16,571 - functions - INFO - BLEU Score: 0.00
2024-12-01 16:01:16,571 - functions - INFO - --------------------------------------------------
2024-12-01 16:01:16,571 - functions - INFO - Source: “Afei, ɛsiane sɛ Honhom Kronkron ahyɛ me no enti, merekɔ Yerusalem a mennim asɛm a ɛbɛto me wɔ hɔ
2024-12-01 16:01:16,571 - functions - INFO - Reference: And now, behold, I—bound in the Spirit—go on to Jerusalem, not knowing the things that will befall me in it
2024-12-01 16:01:16,571 - functions - INFO - Translation: <extra_id_0>
2024-12-01 16:01:16,571 - functions - INFO - BLEU Score: 0.00
2024-12-01 16:01:16,571 - functions - INFO - --------------------------------------------------
2024-12-01 16:01:16,571 - functions - INFO - 
Aggregate Metrics:
2024-12-01 16:01:16,571 - functions - INFO - average_bleu: 0.00
2024-12-01 16:01:16,571 - functions - INFO - max_bleu: 0.00
2024-12-01 16:01:16,571 - functions - INFO - min_bleu: 0.00
2024-12-01 16:01:16,571 - functions - INFO - num_samples: 2
2024-12-01 16:01:16,572 - functions - INFO - Translation pipeline completed successfully
2024-12-01 16:01:16,572 - functions - INFO - === System Information ===
2024-12-01 16:01:16,572 - functions - INFO - PyTorch version: 2.5.0+cu124
2024-12-01 16:01:16,572 - functions - INFO - CUDA available: True
2024-12-01 16:01:16,572 - functions - INFO - Current device: 0
2024-12-01 16:01:16,572 - functions - INFO - Log file location: logs/nllb_TWI_ENGLISH_20241201_160116.log
2024-12-01 16:01:16,572 - functions - INFO - ==================================================
2024-12-01 16:01:16,572 - functions - INFO - Initializing nllb translator...
2024-12-01 16:01:16,572 - functions - INFO - === NLLB Translator Configuration ===
2024-12-01 16:01:16,572 - functions - INFO - Model name: facebook/nllb-200-3.3B
2024-12-01 16:01:16,572 - functions - INFO - Max length: 128
2024-12-01 16:01:16,572 - functions - INFO - Batch size: 16
2024-12-01 16:01:16,572 - functions - INFO - Number of epochs: 1
2024-12-01 16:01:16,572 - functions - INFO - Learning rate: 1e-05
2024-12-01 16:01:16,572 - functions - INFO - Weight decay: 0.01
2024-12-01 16:01:16,572 - functions - INFO - Output directory: ./nllb_TWI_ENGLISH_20241201_160116
2024-12-01 16:01:16,572 - functions - INFO - Source language: twi_Latn
2024-12-01 16:01:16,572 - functions - INFO - Target language: eng_Latn
2024-12-01 16:01:16,572 - functions - INFO - Device: cuda
2024-12-01 16:01:16,572 - functions - INFO - ==================================================
2024-12-01 16:01:19,663 - functions - INFO - Total parameters: 3,344,863,232
2024-12-01 16:01:19,663 - functions - INFO - Trainable parameters: 3,344,863,232
2024-12-01 16:01:19,663 - functions - INFO - Using 4 GPUs
2024-12-01 16:01:19,664 - functions - INFO - Beginning model training procedures...
2024-12-01 16:01:59,963 - functions - INFO - Starting training...
2024-12-01 16:02:03,666 - functions - ERROR - Error during training: CUDA out of memory. Tried to allocate 64.00 MiB. GPU 0 has a total capacity of 93.12 GiB of which 13.12 MiB is free. Including non-PyTorch memory, this process has 93.09 GiB memory in use. Of the allocated memory 91.48 GiB is allocated by PyTorch, and 198.10 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2024-12-01 16:02:03,667 - functions - ERROR - Error in nllb pipeline: CUDA out of memory. Tried to allocate 64.00 MiB. GPU 0 has a total capacity of 93.12 GiB of which 13.12 MiB is free. Including non-PyTorch memory, this process has 93.09 GiB memory in use. Of the allocated memory 91.48 GiB is allocated by PyTorch, and 198.10 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Traceback (most recent call last):
  File "/home/sheriff/research/twi/unified_model_trainer.py", line 45, in run_translation_pipeline
    metrics = translator.train(dataset)
              ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/sheriff/research/twi/functions.py", line 766, in train
    trainer.train()
  File "/home/sheriff/miniconda3/envs/llm_loc/lib/python3.12/site-packages/transformers/trainer.py", line 2052, in train
    return inner_training_loop(
           ^^^^^^^^^^^^^^^^^^^^
  File "/home/sheriff/miniconda3/envs/llm_loc/lib/python3.12/site-packages/transformers/trainer.py", line 2452, in _inner_training_loop
    self.optimizer.step()
  File "/home/sheriff/miniconda3/envs/llm_loc/lib/python3.12/site-packages/torch/optim/lr_scheduler.py", line 137, in wrapper
    return func.__get__(opt, opt.__class__)(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/sheriff/miniconda3/envs/llm_loc/lib/python3.12/site-packages/torch/optim/optimizer.py", line 487, in wrapper
    out = func(*args, **kwargs)
          ^^^^^^^^^^^^^^^^^^^^^
  File "/home/sheriff/miniconda3/envs/llm_loc/lib/python3.12/site-packages/torch/optim/optimizer.py", line 91, in _use_grad
    ret = func(self, *args, **kwargs)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/sheriff/miniconda3/envs/llm_loc/lib/python3.12/site-packages/torch/optim/adamw.py", line 220, in step
    adamw(
  File "/home/sheriff/miniconda3/envs/llm_loc/lib/python3.12/site-packages/torch/optim/optimizer.py", line 154, in maybe_fallback
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/sheriff/miniconda3/envs/llm_loc/lib/python3.12/site-packages/torch/optim/adamw.py", line 782, in adamw
    func(
  File "/home/sheriff/miniconda3/envs/llm_loc/lib/python3.12/site-packages/torch/optim/adamw.py", line 606, in _multi_tensor_adamw
    exp_avg_sq_sqrt = torch._foreach_sqrt(device_exp_avg_sqs)
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 64.00 MiB. GPU 0 has a total capacity of 93.12 GiB of which 13.12 MiB is free. Including non-PyTorch memory, this process has 93.09 GiB memory in use. Of the allocated memory 91.48 GiB is allocated by PyTorch, and 198.10 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2024-12-01 16:02:03,670 - functions - INFO - === System Information ===
2024-12-01 16:02:03,670 - functions - INFO - PyTorch version: 2.5.0+cu124
2024-12-01 16:02:03,670 - functions - INFO - CUDA available: True
2024-12-01 16:02:03,670 - functions - INFO - Current device: 0
2024-12-01 16:02:03,670 - functions - INFO - Log file location: logs/opt_TWI_ENGLISH_20241201_160203.log
2024-12-01 16:02:03,670 - functions - INFO - ==================================================
2024-12-01 16:02:03,670 - functions - INFO - Initializing opt translator...
2024-12-01 16:02:30,722 - functions - INFO - === OPT Translator Configuration ===
2024-12-01 16:02:30,722 - functions - INFO - Model name: facebook/opt-1.3b
2024-12-01 16:02:30,723 - functions - INFO - Max length: 128
2024-12-01 16:02:30,723 - functions - INFO - Batch size: 16
2024-12-01 16:02:30,723 - functions - INFO - Number of epochs: 1
2024-12-01 16:02:30,723 - functions - INFO - Learning rate: 1e-05
2024-12-01 16:02:30,723 - functions - INFO - Weight decay: 0.01
2024-12-01 16:02:30,723 - functions - INFO - Source language: twi
2024-12-01 16:02:30,723 - functions - INFO - Target language: eng
2024-12-01 16:02:30,723 - functions - INFO - Device: cuda
2024-12-01 16:02:30,725 - functions - INFO - Total parameters: 1,315,758,080
2024-12-01 16:02:30,725 - functions - INFO - Trainable parameters: 1,315,758,080
2024-12-01 16:02:30,725 - functions - INFO - ==================================================
2024-12-01 16:02:30,725 - functions - INFO - Beginning model training procedures...
2024-12-01 16:02:41,473 - functions - ERROR - Error during training: TrainingArguments.__init__() got an unexpected keyword argument 'predict_with_generate'
2024-12-01 16:02:41,473 - functions - ERROR - Error in opt pipeline: TrainingArguments.__init__() got an unexpected keyword argument 'predict_with_generate'
Traceback (most recent call last):
  File "/home/sheriff/research/twi/unified_model_trainer.py", line 45, in run_translation_pipeline
    metrics = translator.train(dataset)
              ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/sheriff/research/twi/functions.py", line 1359, in train
    training_args = TrainingArguments(
                    ^^^^^^^^^^^^^^^^^^
TypeError: TrainingArguments.__init__() got an unexpected keyword argument 'predict_with_generate'
2024-12-01 16:02:41,474 - functions - INFO - === System Information ===
2024-12-01 16:02:41,474 - functions - INFO - PyTorch version: 2.5.0+cu124
2024-12-01 16:02:41,474 - functions - INFO - CUDA available: True
2024-12-01 16:02:41,474 - functions - INFO - Current device: 0
2024-12-01 16:02:41,474 - functions - INFO - Log file location: logs/xglm_TWI_ENGLISH_20241201_160241.log
2024-12-01 16:02:41,474 - functions - INFO - ==================================================
2024-12-01 16:02:41,474 - functions - INFO - Initializing xglm translator...
2024-12-01 16:03:02,015 - functions - INFO - === XGLM Translator Configuration ===
2024-12-01 16:03:02,016 - functions - INFO - Model name: facebook/xglm-564M
2024-12-01 16:03:02,016 - functions - INFO - Max length: 128
2024-12-01 16:03:02,016 - functions - INFO - Batch size: 16
2024-12-01 16:03:02,016 - functions - INFO - Number of epochs: 1
2024-12-01 16:03:02,016 - functions - INFO - Learning rate: 1e-05
2024-12-01 16:03:02,016 - functions - INFO - Weight decay: 0.01
2024-12-01 16:03:02,016 - functions - INFO - Source language: twi
2024-12-01 16:03:02,016 - functions - INFO - Target language: eng
2024-12-01 16:03:02,016 - functions - INFO - Device: cuda
2024-12-01 16:03:02,018 - functions - INFO - Total parameters: 564,463,616
2024-12-01 16:03:02,018 - functions - INFO - Trainable parameters: 564,463,616
2024-12-01 16:03:02,018 - functions - INFO - ==================================================
2024-12-01 16:03:02,035 - functions - INFO - Beginning model training procedures...
2024-12-01 16:03:06,065 - functions - INFO - Starting training...
2024-12-01 16:03:10,382 - functions - ERROR - Error during training: Input length of input_ids is 128, but `max_length` is set to 20. This can lead to unexpected behavior. You should consider increasing `max_length` or, better yet, setting `max_new_tokens`.
2024-12-01 16:03:10,382 - functions - ERROR - Error in xglm pipeline: Input length of input_ids is 128, but `max_length` is set to 20. This can lead to unexpected behavior. You should consider increasing `max_length` or, better yet, setting `max_new_tokens`.
Traceback (most recent call last):
  File "/home/sheriff/research/twi/unified_model_trainer.py", line 45, in run_translation_pipeline
    metrics = translator.train(dataset)
              ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/sheriff/research/twi/functions.py", line 2038, in train
    trainer.train()
  File "/home/sheriff/miniconda3/envs/llm_loc/lib/python3.12/site-packages/transformers/trainer.py", line 2052, in train
    return inner_training_loop(
           ^^^^^^^^^^^^^^^^^^^^
  File "/home/sheriff/miniconda3/envs/llm_loc/lib/python3.12/site-packages/transformers/trainer.py", line 2487, in _inner_training_loop
    self._maybe_log_save_evaluate(tr_loss, grad_norm, model, trial, epoch, ignore_keys_for_eval)
  File "/home/sheriff/miniconda3/envs/llm_loc/lib/python3.12/site-packages/transformers/trainer.py", line 2915, in _maybe_log_save_evaluate
    metrics = self._evaluate(trial, ignore_keys_for_eval)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/sheriff/miniconda3/envs/llm_loc/lib/python3.12/site-packages/transformers/trainer.py", line 2872, in _evaluate
    metrics = self.evaluate(ignore_keys=ignore_keys_for_eval)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/sheriff/miniconda3/envs/llm_loc/lib/python3.12/site-packages/transformers/trainer_seq2seq.py", line 180, in evaluate
    return super().evaluate(eval_dataset, ignore_keys=ignore_keys, metric_key_prefix=metric_key_prefix)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/sheriff/miniconda3/envs/llm_loc/lib/python3.12/site-packages/transformers/trainer.py", line 3868, in evaluate
    output = eval_loop(
             ^^^^^^^^^^
  File "/home/sheriff/miniconda3/envs/llm_loc/lib/python3.12/site-packages/transformers/trainer.py", line 4061, in evaluation_loop
    losses, logits, labels = self.prediction_step(model, inputs, prediction_loss_only, ignore_keys=ignore_keys)
                             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/sheriff/miniconda3/envs/llm_loc/lib/python3.12/site-packages/transformers/trainer_seq2seq.py", line 310, in prediction_step
    generated_tokens = self.model.generate(**generation_inputs, **gen_kwargs)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/sheriff/miniconda3/envs/llm_loc/lib/python3.12/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/sheriff/miniconda3/envs/llm_loc/lib/python3.12/site-packages/transformers/generation/utils.py", line 1905, in generate
    self._validate_generated_length(generation_config, input_ids_length, has_default_max_length)
  File "/home/sheriff/miniconda3/envs/llm_loc/lib/python3.12/site-packages/transformers/generation/utils.py", line 1228, in _validate_generated_length
    raise ValueError(
ValueError: Input length of input_ids is 128, but `max_length` is set to 20. This can lead to unexpected behavior. You should consider increasing `max_length` or, better yet, setting `max_new_tokens`.
