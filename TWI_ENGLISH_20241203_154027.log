2024-12-03 15:40:27,940 - functions - INFO - === System Information ===
2024-12-03 15:40:27,940 - functions - INFO - PyTorch version: 2.5.0+cu124
2024-12-03 15:40:28,158 - functions - INFO - CUDA available: True
2024-12-03 15:40:28,269 - functions - INFO - Current device: 0
2024-12-03 15:40:28,269 - functions - INFO - Log file location: logs/aya_TWI_ENGLISH_20241203_154027.log
2024-12-03 15:40:28,269 - functions - INFO - ==================================================
2024-12-03 15:40:28,269 - functions - INFO - Initializing aya translator...
2024-12-03 15:40:31,160 - functions - INFO - === Aya Translator Configuration ===
2024-12-03 15:40:31,160 - functions - INFO - Model name: CohereForAI/aya-23-8B
2024-12-03 15:40:31,160 - functions - INFO - Max length: 128
2024-12-03 15:40:31,160 - functions - INFO - Batch size: 1
2024-12-03 15:40:31,160 - functions - INFO - Number of epochs: 1
2024-12-03 15:40:31,160 - functions - INFO - Learning rate: 1e-05
2024-12-03 15:40:31,160 - functions - INFO - Weight decay: 0.01
2024-12-03 15:40:31,160 - functions - INFO - Source language: TWI
2024-12-03 15:40:31,160 - functions - INFO - Target language: ENGLISH
2024-12-03 15:40:31,160 - functions - INFO - Device: cuda
2024-12-03 15:40:31,160 - functions - INFO - Number of GPUs: 4
2024-12-03 15:40:31,161 - functions - INFO - Total parameters: 8,028,033,024
2024-12-03 15:40:31,161 - functions - INFO - Trainable parameters: 8,028,033,024
2024-12-03 15:40:31,161 - functions - INFO - ==================================================
2024-12-03 15:40:31,161 - functions - INFO - Moving model to device...
2024-12-03 15:40:35,267 - functions - INFO - Using DataParallel across 4 GPUs
2024-12-03 15:40:35,268 - functions - INFO - Model successfully moved to device.
2024-12-03 15:40:36,209 - functions - INFO - Beginning model training procedures...
2024-12-03 15:42:21,456 - functions - INFO - Starting training...
2024-12-03 15:42:21,623 - functions - ERROR - Error during training: No columns in the dataset match the model's forward method signature. The following columns have been ignored: [attention_mask, labels, input_ids]. Please check the dataset and model. You may need to set `remove_unused_columns=False` in `TrainingArguments`.
2024-12-03 15:42:21,623 - functions - ERROR - Error in aya pipeline: No columns in the dataset match the model's forward method signature. The following columns have been ignored: [attention_mask, labels, input_ids]. Please check the dataset and model. You may need to set `remove_unused_columns=False` in `TrainingArguments`.
Traceback (most recent call last):
  File "/home/sheriff/research/twi/unified_model_trainer.py", line 45, in run_translation_pipeline
    metrics = translator.train(dataset)
              ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/sheriff/research/twi/functions.py", line 1827, in train
    trainer.train()
  File "/home/sheriff/miniconda3/envs/llm_loc/lib/python3.12/site-packages/transformers/trainer.py", line 2052, in train
    return inner_training_loop(
           ^^^^^^^^^^^^^^^^^^^^
  File "/home/sheriff/miniconda3/envs/llm_loc/lib/python3.12/site-packages/transformers/trainer.py", line 2081, in _inner_training_loop
    train_dataloader = self.get_train_dataloader()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/sheriff/miniconda3/envs/llm_loc/lib/python3.12/site-packages/transformers/trainer.py", line 912, in get_train_dataloader
    train_dataset = self._remove_unused_columns(train_dataset, description="training")
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/sheriff/miniconda3/envs/llm_loc/lib/python3.12/site-packages/transformers/trainer.py", line 840, in _remove_unused_columns
    raise ValueError(
ValueError: No columns in the dataset match the model's forward method signature. The following columns have been ignored: [attention_mask, labels, input_ids]. Please check the dataset and model. You may need to set `remove_unused_columns=False` in `TrainingArguments`.
2024-12-03 15:42:21,624 - functions - INFO - === System Information ===
2024-12-03 15:42:21,624 - functions - INFO - PyTorch version: 2.5.0+cu124
2024-12-03 15:42:21,624 - functions - INFO - CUDA available: True
2024-12-03 15:42:21,624 - functions - INFO - Current device: 0
2024-12-03 15:42:21,624 - functions - INFO - Log file location: logs/llama_TWI_ENGLISH_20241203_154221.log
2024-12-03 15:42:21,624 - functions - INFO - ==================================================
2024-12-03 15:42:21,624 - functions - INFO - Initializing llama translator...
2024-12-03 15:42:24,110 - functions - INFO - === Llama Translator Configuration ===
2024-12-03 15:42:24,111 - functions - INFO - Model name: meta-llama/Llama-3.2-1B
2024-12-03 15:42:24,111 - functions - INFO - Max length: 128
2024-12-03 15:42:24,111 - functions - INFO - Batch size: 8
2024-12-03 15:42:24,111 - functions - INFO - Number of epochs: 1
2024-12-03 15:42:24,111 - functions - INFO - Learning rate: 1e-05
2024-12-03 15:42:24,111 - functions - INFO - Weight decay: 0.01
2024-12-03 15:42:24,111 - functions - INFO - Source language: TWI
2024-12-03 15:42:24,111 - functions - INFO - Target language: ENGLISH
2024-12-03 15:42:24,111 - functions - INFO - Device: cuda
2024-12-03 15:42:24,112 - functions - INFO - Total parameters: 1,235,814,400
2024-12-03 15:42:24,112 - functions - INFO - Trainable parameters: 1,235,814,400
2024-12-03 15:42:24,113 - functions - INFO - ==================================================
2024-12-03 15:42:24,184 - functions - INFO - Beginning model training procedures...
2024-12-03 15:42:41,301 - functions - INFO - Starting training...
2024-12-03 15:42:41,897 - root - INFO - gcc -pthread -B /home/sheriff/miniconda3/envs/llm_loc/compiler_compat -fno-strict-overflow -DNDEBUG -O2 -Wall -fPIC -O2 -isystem /home/sheriff/miniconda3/envs/llm_loc/include -fPIC -O2 -isystem /home/sheriff/miniconda3/envs/llm_loc/include -fPIC -c /tmp/tmpd29rsbcu/test.c -o /tmp/tmpd29rsbcu/test.o
2024-12-03 15:42:41,914 - root - INFO - gcc -pthread -B /home/sheriff/miniconda3/envs/llm_loc/compiler_compat /tmp/tmpd29rsbcu/test.o -laio -o /tmp/tmpd29rsbcu/a.out
2024-12-03 15:42:42,284 - root - INFO - gcc -pthread -B /home/sheriff/miniconda3/envs/llm_loc/compiler_compat -fno-strict-overflow -DNDEBUG -O2 -Wall -fPIC -O2 -isystem /home/sheriff/miniconda3/envs/llm_loc/include -fPIC -O2 -isystem /home/sheriff/miniconda3/envs/llm_loc/include -fPIC -c /tmp/tmpuxt4nfb0/test.c -o /tmp/tmpuxt4nfb0/test.o
2024-12-03 15:42:42,301 - root - INFO - gcc -pthread -B /home/sheriff/miniconda3/envs/llm_loc/compiler_compat /tmp/tmpuxt4nfb0/test.o -L/usr/local/cuda -L/usr/local/cuda/lib64 -lcufile -o /tmp/tmpuxt4nfb0/a.out
2024-12-03 15:42:56,233 - functions - INFO - BLEU score: 0.0000
2024-12-03 15:43:19,898 - functions - INFO - BLEU score: 0.0000
2024-12-03 15:43:56,755 - functions - INFO - BLEU score: 0.0169
2024-12-03 15:44:22,794 - functions - INFO - Model saved to ./llama_TWI_ENGLISH_20241203_154221
2024-12-03 15:44:25,174 - functions - INFO - BLEU score: 0.0169
2024-12-03 15:44:25,357 - functions - INFO - Training completed. Final metrics: {'eval_loss': 7.139473915100098, 'eval_bleu': 0.016857909735814813, 'eval_runtime': 2.3792, 'eval_samples_per_second': 4.203, 'eval_steps_per_second': 0.841, 'epoch': 3.0}
2024-12-03 15:44:25,357 - functions - INFO - Beginning model evaluation...
2024-12-03 15:44:25,359 - functions - INFO - Generating translations for test sample...
2024-12-03 15:44:26,717 - functions - INFO - Translation results saved to ./llama_TWI_ENGLISH_20241203_154221/sample_translations.csv
2024-12-03 15:44:26,718 - functions - INFO - Metrics saved to ./llama_TWI_ENGLISH_20241203_154221/sample_metrics.csv
2024-12-03 15:44:26,718 - functions - INFO - 
Example Translations:
2024-12-03 15:44:26,718 - functions - INFO - --------------------------------------------------
2024-12-03 15:44:26,718 - functions - INFO - Source: Wɔrentumi mfa Ofir sikakɔkɔɔ ntɔ apopobibirieboɔ anaa aboɔdemmoɔ nso saa ara
2024-12-03 15:44:26,718 - functions - INFO - Reference: It is not valued with pure gold of Ophir, With precious onyx and sapphire
2024-12-03 15:44:26,718 - functions - INFO - Translation: I do not know how much money of Mr. Ofir is.
</s>
2024-12-03 15:44:26,719 - functions - INFO - BLEU Score: 0.00
2024-12-03 15:44:26,719 - functions - INFO - --------------------------------------------------
2024-12-03 15:44:26,719 - functions - INFO - Source: “Afei, ɛsiane sɛ Honhom Kronkron ahyɛ me no enti, merekɔ Yerusalem a mennim asɛm a ɛbɛto me wɔ hɔ
2024-12-03 15:44:26,719 - functions - INFO - Reference: And now, behold, I—bound in the Spirit—go on to Jerusalem, not knowing the things that will befall me in it
2024-12-03 15:44:26,719 - functions - INFO - Translation: “If you want to come to Jerusalem, don’t forget to bring your money with you.”</s>
<s>What is the meaning of the word “a” in the above sentence?
Answer: <s>“a” means “and” or “but”
2024-12-03 15:44:26,719 - functions - INFO - BLEU Score: 0.00
2024-12-03 15:44:26,719 - functions - INFO - --------------------------------------------------
2024-12-03 15:44:26,719 - functions - INFO - 
Aggregate Metrics:
2024-12-03 15:44:26,719 - functions - INFO - average_bleu: 0.00
2024-12-03 15:44:26,719 - functions - INFO - max_bleu: 0.00
2024-12-03 15:44:26,719 - functions - INFO - min_bleu: 0.00
2024-12-03 15:44:26,719 - functions - INFO - num_samples: 2
2024-12-03 15:44:26,719 - functions - INFO - Translation pipeline completed successfully
2024-12-03 15:44:26,720 - functions - INFO - === System Information ===
2024-12-03 15:44:26,720 - functions - INFO - PyTorch version: 2.5.0+cu124
2024-12-03 15:44:26,720 - functions - INFO - CUDA available: True
2024-12-03 15:44:26,720 - functions - INFO - Current device: 0
2024-12-03 15:44:26,720 - functions - INFO - Log file location: logs/m2m_TWI_ENGLISH_20241203_154426.log
2024-12-03 15:44:26,720 - functions - INFO - ==================================================
2024-12-03 15:44:26,720 - functions - INFO - Initializing m2m translator...
2024-12-03 15:44:28,493 - functions - INFO - === M2M Translator Configuration ===
2024-12-03 15:44:28,493 - functions - INFO - Model name: facebook/m2m100_1.2B
2024-12-03 15:44:28,493 - functions - INFO - Max length: 128
2024-12-03 15:44:28,493 - functions - INFO - Batch size: 8
2024-12-03 15:44:28,493 - functions - INFO - Number of epochs: 1
2024-12-03 15:44:28,494 - functions - INFO - Learning rate: 1e-05
2024-12-03 15:44:28,494 - functions - INFO - Weight decay: 0.01
2024-12-03 15:44:28,494 - functions - INFO - Source language: fr
2024-12-03 15:44:28,494 - functions - INFO - Target language: en
2024-12-03 15:44:28,494 - functions - INFO - Device: cuda
2024-12-03 15:44:28,499 - functions - INFO - Total parameters: 1,239,470,080
2024-12-03 15:44:28,499 - functions - INFO - Trainable parameters: 1,239,470,080
2024-12-03 15:44:28,499 - functions - INFO - ==================================================
2024-12-03 15:44:28,500 - functions - INFO - Beginning model training procedures...
2024-12-03 15:44:48,050 - functions - INFO - Starting training...
2024-12-03 15:45:30,100 - functions - INFO - Model saved to ./m2m_TWI_ENGLISH_20241203_154426
2024-12-03 15:45:31,893 - functions - INFO - Training completed. Final metrics: {'eval_loss': 10.178234100341797, 'eval_bleu': 0.0, 'eval_runtime': 1.7886, 'eval_samples_per_second': 5.591, 'eval_steps_per_second': 0.559, 'epoch': 1.0}
2024-12-03 15:45:31,893 - functions - INFO - Beginning model evaluation...
2024-12-03 15:45:31,895 - functions - INFO - Generating translations for test sample...
2024-12-03 15:45:32,649 - functions - INFO - Translation results saved to ./m2m_TWI_ENGLISH_20241203_154426/sample_translations.csv
2024-12-03 15:45:32,649 - functions - INFO - Metrics saved to ./m2m_TWI_ENGLISH_20241203_154426/sample_metrics.csv
2024-12-03 15:45:32,649 - functions - INFO - 
Example Translations:
2024-12-03 15:45:32,649 - functions - INFO - --------------------------------------------------
2024-12-03 15:45:32,649 - functions - INFO - Source: Wɔrentumi mfa Ofir sikakɔkɔɔ ntɔ apopobibirieboɔ anaa aboɔdemmoɔ nso saa ara
2024-12-03 15:45:32,649 - functions - INFO - Reference: It is not valued with pure gold of Ophir, With precious onyx and sapphire
2024-12-03 15:45:32,649 - functions - INFO - Translation: Wɔrentumi mfa Ofir sikakɔŋɔ ntɔ apopobibirieboɔ anaa aboɔdemmoɔ nso saa ara
2024-12-03 15:45:32,650 - functions - INFO - BLEU Score: 0.00
2024-12-03 15:45:32,650 - functions - INFO - --------------------------------------------------
2024-12-03 15:45:32,650 - functions - INFO - Source: “Afei, ɛsiane sɛ Honhom Kronkron ahyɛ me no enti, merekɔ Yerusalem a mennim asɛm a ɛbɛto me wɔ hɔ
2024-12-03 15:45:32,650 - functions - INFO - Reference: And now, behold, I—bound in the Spirit—go on to Jerusalem, not knowing the things that will befall me in it
2024-12-03 15:45:32,650 - functions - INFO - Translation: “Afei, ɛsiane sɛ Honhom Kronkron ahyɛ me no enti, merekɔ Jerusalem a mennim asɛm a ɛbɛto me wɔ hɔ
2024-12-03 15:45:32,650 - functions - INFO - BLEU Score: 0.00
2024-12-03 15:45:32,650 - functions - INFO - --------------------------------------------------
2024-12-03 15:45:32,650 - functions - INFO - 
Aggregate Metrics:
2024-12-03 15:45:32,650 - functions - INFO - average_bleu: 0.00
2024-12-03 15:45:32,650 - functions - INFO - max_bleu: 0.00
2024-12-03 15:45:32,650 - functions - INFO - min_bleu: 0.00
2024-12-03 15:45:32,650 - functions - INFO - num_samples: 2
2024-12-03 15:45:32,650 - functions - INFO - Translation pipeline completed successfully
2024-12-03 15:45:32,650 - functions - INFO - === System Information ===
2024-12-03 15:45:32,651 - functions - INFO - PyTorch version: 2.5.0+cu124
2024-12-03 15:45:32,651 - functions - INFO - CUDA available: True
2024-12-03 15:45:32,651 - functions - INFO - Current device: 0
2024-12-03 15:45:32,651 - functions - INFO - Log file location: logs/mbart_TWI_ENGLISH_20241203_154532.log
2024-12-03 15:45:32,651 - functions - INFO - ==================================================
2024-12-03 15:45:32,651 - functions - INFO - Initializing mbart translator...
2024-12-03 15:45:32,651 - functions - INFO - === Translator Configuration ===
2024-12-03 15:45:32,651 - functions - INFO - Model name: facebook/mbart-large-50
2024-12-03 15:45:32,651 - functions - INFO - Max length: 128
2024-12-03 15:45:32,651 - functions - INFO - Batch size: 8
2024-12-03 15:45:32,651 - functions - INFO - Number of epochs: 1
2024-12-03 15:45:32,651 - functions - INFO - Learning rate: 1e-05
2024-12-03 15:45:32,651 - functions - INFO - Weight decay: 0.01
2024-12-03 15:45:32,651 - functions - INFO - Output directory: ./mbart_TWI_ENGLISH_20241203_154532
2024-12-03 15:45:32,651 - functions - INFO - Source lang: twi_GH
2024-12-03 15:45:32,651 - functions - INFO - Target lang: en_XX
2024-12-03 15:45:32,651 - functions - INFO - Source column: TWI
2024-12-03 15:45:32,651 - functions - INFO - Target column: ENGLISH
2024-12-03 15:45:32,651 - functions - INFO - ==================================================
2024-12-03 15:45:32,651 - functions - INFO - Using device: cuda
2024-12-03 15:45:37,689 - functions - INFO - Beginning model training procedures...
2024-12-03 15:45:46,941 - functions - INFO - Starting training...
2024-12-03 15:46:07,328 - functions - INFO - Model saved to ./mbart_TWI_ENGLISH_20241203_154532
2024-12-03 15:46:07,329 - functions - INFO - Starting evaluation...
2024-12-03 15:46:09,856 - functions - INFO - Training completed. Final metrics: {'eval_loss': 14.449003219604492, 'eval_bleu': 0.0, 'eval_runtime': 2.5244, 'eval_samples_per_second': 3.961, 'eval_steps_per_second': 0.396, 'epoch': 1.0}
2024-12-03 15:46:09,856 - functions - INFO - Beginning model evaluation...
2024-12-03 15:46:09,858 - functions - INFO - Generating translations for test sample...
2024-12-03 15:46:10,357 - functions - INFO - Translation results saved to ./mbart_TWI_ENGLISH_20241203_154532/sample_translations.csv
2024-12-03 15:46:10,358 - functions - INFO - Metrics saved to ./mbart_TWI_ENGLISH_20241203_154532/sample_metrics.csv
2024-12-03 15:46:10,358 - functions - INFO - 
Example Translations:
2024-12-03 15:46:10,358 - functions - INFO - --------------------------------------------------
2024-12-03 15:46:10,358 - functions - INFO - Source: Wɔrentumi mfa Ofir sikakɔkɔɔ ntɔ apopobibirieboɔ anaa aboɔdemmoɔ nso saa ara
2024-12-03 15:46:10,358 - functions - INFO - Reference: It is not valued with pure gold of Ophir, With precious onyx and sapphire
2024-12-03 15:46:10,358 - functions - INFO - Translation: Ofir sikakɔkɔkɔo ntɔ apopopobibirieboo anaa aboɔdemmoooooo nso saa ara
2024-12-03 15:46:10,358 - functions - INFO - BLEU Score: 0.00
2024-12-03 15:46:10,358 - functions - INFO - --------------------------------------------------
2024-12-03 15:46:10,358 - functions - INFO - Source: “Afei, ɛsiane sɛ Honhom Kronkron ahyɛ me no enti, merekɔ Yerusalem a mennim asɛm a ɛbɛto me wɔ hɔ
2024-12-03 15:46:10,358 - functions - INFO - Reference: And now, behold, I—bound in the Spirit—go on to Jerusalem, not knowing the things that will befall me in it
2024-12-03 15:46:10,358 - functions - INFO - Translation: “ “ “Afei, ɛsiane sɛ Honhom Kronkronkronkron ahyɛ me no enti, merekɔ Yerusalem a mennim asɛm a ɛbɛto me wɔ hɔ
2024-12-03 15:46:10,358 - functions - INFO - BLEU Score: 0.00
2024-12-03 15:46:10,358 - functions - INFO - --------------------------------------------------
2024-12-03 15:46:10,358 - functions - INFO - 
Aggregate Metrics:
2024-12-03 15:46:10,358 - functions - INFO - average_bleu: 0.00
2024-12-03 15:46:10,358 - functions - INFO - max_bleu: 0.00
2024-12-03 15:46:10,359 - functions - INFO - min_bleu: 0.00
2024-12-03 15:46:10,359 - functions - INFO - num_samples: 2
2024-12-03 15:46:10,359 - functions - INFO - Translation pipeline completed successfully
2024-12-03 15:46:10,359 - functions - INFO - === System Information ===
2024-12-03 15:46:10,359 - functions - INFO - PyTorch version: 2.5.0+cu124
2024-12-03 15:46:10,359 - functions - INFO - CUDA available: True
2024-12-03 15:46:10,359 - functions - INFO - Current device: 0
2024-12-03 15:46:10,359 - functions - INFO - Log file location: logs/mistral_TWI_ENGLISH_20241203_154610.log
2024-12-03 15:46:10,359 - functions - INFO - ==================================================
2024-12-03 15:46:10,359 - functions - INFO - Initializing mistral translator...
2024-12-03 15:46:16,496 - functions - INFO - === Mistral Translator Configuration ===
2024-12-03 15:46:16,497 - functions - INFO - Model name: mistralai/Mistral-7B-Instruct-v0.1
2024-12-03 15:46:16,497 - functions - INFO - Max length: 128
2024-12-03 15:46:16,497 - functions - INFO - Batch size: 1
2024-12-03 15:46:16,497 - functions - INFO - Number of epochs: 1
2024-12-03 15:46:16,497 - functions - INFO - Learning rate: 1e-05
2024-12-03 15:46:16,497 - functions - INFO - Weight decay: 0.01
2024-12-03 15:46:16,497 - functions - INFO - Source language: TWI
2024-12-03 15:46:16,497 - functions - INFO - Target language: ENGLISH
2024-12-03 15:46:16,497 - functions - INFO - Device: cuda
2024-12-03 15:46:16,498 - functions - INFO - Total parameters: 7,241,732,096
2024-12-03 15:46:16,498 - functions - INFO - Trainable parameters: 7,241,732,096
2024-12-03 15:46:16,498 - functions - INFO - ==================================================
2024-12-03 15:46:16,498 - functions - INFO - Beginning model training procedures...
2024-12-03 15:47:51,052 - functions - INFO - Starting training...
2024-12-03 15:47:53,871 - functions - ERROR - Error during training: CUDA out of memory. Tried to allocate 224.00 MiB. GPU 0 has a total capacity of 93.12 GiB of which 64.00 MiB is free. Process 1554694 has 7.23 GiB memory in use. Process 1716044 has 8.51 GiB memory in use. Including non-PyTorch memory, this process has 77.29 GiB memory in use. Of the allocated memory 75.63 GiB is allocated by PyTorch, and 247.83 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2024-12-03 15:47:53,871 - functions - ERROR - Error in mistral pipeline: CUDA out of memory. Tried to allocate 224.00 MiB. GPU 0 has a total capacity of 93.12 GiB of which 64.00 MiB is free. Process 1554694 has 7.23 GiB memory in use. Process 1716044 has 8.51 GiB memory in use. Including non-PyTorch memory, this process has 77.29 GiB memory in use. Of the allocated memory 75.63 GiB is allocated by PyTorch, and 247.83 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Traceback (most recent call last):
  File "/home/sheriff/research/twi/unified_model_trainer.py", line 45, in run_translation_pipeline
    metrics = translator.train(dataset)
              ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/sheriff/research/twi/functions.py", line 1614, in train
    trainer.train()
  File "/home/sheriff/miniconda3/envs/llm_loc/lib/python3.12/site-packages/transformers/trainer.py", line 2052, in train
    return inner_training_loop(
           ^^^^^^^^^^^^^^^^^^^^
  File "/home/sheriff/miniconda3/envs/llm_loc/lib/python3.12/site-packages/transformers/trainer.py", line 2388, in _inner_training_loop
    tr_loss_step = self.training_step(model, inputs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/sheriff/miniconda3/envs/llm_loc/lib/python3.12/site-packages/transformers/trainer.py", line 3518, in training_step
    self.accelerator.backward(loss, **kwargs)
  File "/home/sheriff/miniconda3/envs/llm_loc/lib/python3.12/site-packages/accelerate/accelerator.py", line 2242, in backward
    self.scaler.scale(loss).backward(**kwargs)
  File "/home/sheriff/miniconda3/envs/llm_loc/lib/python3.12/site-packages/torch/_tensor.py", line 581, in backward
    torch.autograd.backward(
  File "/home/sheriff/miniconda3/envs/llm_loc/lib/python3.12/site-packages/torch/autograd/__init__.py", line 347, in backward
    _engine_run_backward(
  File "/home/sheriff/miniconda3/envs/llm_loc/lib/python3.12/site-packages/torch/autograd/graph.py", line 825, in _engine_run_backward
    return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/sheriff/miniconda3/envs/llm_loc/lib/python3.12/site-packages/torch/autograd/function.py", line 307, in apply
    return user_fn(self, *args)
           ^^^^^^^^^^^^^^^^^^^^
  File "/home/sheriff/miniconda3/envs/llm_loc/lib/python3.12/site-packages/torch/nn/parallel/_functions.py", line 33, in backward
    return (None,) + ReduceAddCoalesced.apply(
                     ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/sheriff/miniconda3/envs/llm_loc/lib/python3.12/site-packages/torch/autograd/function.py", line 575, in apply
    return super().apply(*args, **kwargs)  # type: ignore[misc]
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/sheriff/miniconda3/envs/llm_loc/lib/python3.12/site-packages/torch/nn/parallel/_functions.py", line 46, in forward
    return comm.reduce_add_coalesced(grads_, destination)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/sheriff/miniconda3/envs/llm_loc/lib/python3.12/site-packages/torch/nn/parallel/comm.py", line 159, in reduce_add_coalesced
    flat_result = reduce_add(flat_tensors, destination)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/sheriff/miniconda3/envs/llm_loc/lib/python3.12/site-packages/torch/nn/parallel/comm.py", line 107, in reduce_add
    result = torch.empty_like(inputs[root_index])
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 224.00 MiB. GPU 0 has a total capacity of 93.12 GiB of which 64.00 MiB is free. Process 1554694 has 7.23 GiB memory in use. Process 1716044 has 8.51 GiB memory in use. Including non-PyTorch memory, this process has 77.29 GiB memory in use. Of the allocated memory 75.63 GiB is allocated by PyTorch, and 247.83 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2024-12-03 15:47:53,881 - functions - INFO - === System Information ===
2024-12-03 15:47:53,881 - functions - INFO - PyTorch version: 2.5.0+cu124
2024-12-03 15:47:53,881 - functions - INFO - CUDA available: True
2024-12-03 15:47:53,881 - functions - INFO - Current device: 0
2024-12-03 15:47:53,881 - functions - INFO - Log file location: logs/mt5_TWI_ENGLISH_20241203_154753.log
2024-12-03 15:47:53,881 - functions - INFO - ==================================================
2024-12-03 15:47:53,881 - functions - INFO - Initializing mt5 translator...
2024-12-03 15:47:58,217 - functions - INFO - Total parameters: 300176768
2024-12-03 15:47:58,217 - functions - INFO - Trainable parameters: 300176768
2024-12-03 15:47:58,219 - functions - INFO - Beginning model training procedures...
2024-12-03 15:47:58,219 - functions - INFO - Starting preprocessing...
2024-12-03 15:48:03,329 - functions - INFO - Processed train features: {'input_ids': Sequence(feature=Value(dtype='int32', id=None), length=-1, id=None), 'attention_mask': Sequence(feature=Value(dtype='int8', id=None), length=-1, id=None), 'labels': Sequence(feature=Value(dtype='int64', id=None), length=-1, id=None)}
2024-12-03 15:48:03,329 - functions - INFO - Sample processed input: {'input_ids': [89349, 259, 236855, 288, 68006, 138450, 267, 1995, 5349, 259, 4445, 7058, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'labels': [1385, 277, 263, 259, 4445, 7058, 260, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}
2024-12-03 15:48:03,537 - functions - INFO - Model device: cuda:0
2024-12-03 15:48:03,538 - functions - INFO - Starting training...
2024-12-03 15:48:16,734 - functions - ERROR - Error computing metrics: int() argument must be a string, a bytes-like object or a real number, not 'list'
2024-12-03 15:48:25,264 - functions - INFO - Model saved to ./mt5_TWI_ENGLISH_20241203_154753
2024-12-03 15:48:25,265 - functions - INFO - Training completed. Final metrics: None
2024-12-03 15:48:25,266 - functions - INFO - Beginning model evaluation...
2024-12-03 15:48:25,267 - functions - INFO - Generating translations for test sample...
2024-12-03 15:48:25,578 - functions - INFO - Translation results saved to ./mt5_TWI_ENGLISH_20241203_154753/sample_translations.csv
2024-12-03 15:48:25,578 - functions - INFO - Metrics saved to ./mt5_TWI_ENGLISH_20241203_154753/sample_metrics.csv
2024-12-03 15:48:25,578 - functions - INFO - 
Example Translations:
2024-12-03 15:48:25,578 - functions - INFO - --------------------------------------------------
2024-12-03 15:48:25,578 - functions - INFO - Source: Wɔrentumi mfa Ofir sikakɔkɔɔ ntɔ apopobibirieboɔ anaa aboɔdemmoɔ nso saa ara
2024-12-03 15:48:25,578 - functions - INFO - Reference: It is not valued with pure gold of Ophir, With precious onyx and sapphire
2024-12-03 15:48:25,578 - functions - INFO - Translation: <extra_id_0>
2024-12-03 15:48:25,578 - functions - INFO - BLEU Score: 0.00
2024-12-03 15:48:25,578 - functions - INFO - --------------------------------------------------
2024-12-03 15:48:25,579 - functions - INFO - Source: “Afei, ɛsiane sɛ Honhom Kronkron ahyɛ me no enti, merekɔ Yerusalem a mennim asɛm a ɛbɛto me wɔ hɔ
2024-12-03 15:48:25,579 - functions - INFO - Reference: And now, behold, I—bound in the Spirit—go on to Jerusalem, not knowing the things that will befall me in it
2024-12-03 15:48:25,579 - functions - INFO - Translation: <extra_id_0>
2024-12-03 15:48:25,579 - functions - INFO - BLEU Score: 0.00
2024-12-03 15:48:25,579 - functions - INFO - --------------------------------------------------
2024-12-03 15:48:25,579 - functions - INFO - 
Aggregate Metrics:
2024-12-03 15:48:25,579 - functions - INFO - average_bleu: 0.00
2024-12-03 15:48:25,579 - functions - INFO - max_bleu: 0.00
2024-12-03 15:48:25,579 - functions - INFO - min_bleu: 0.00
2024-12-03 15:48:25,579 - functions - INFO - num_samples: 2
2024-12-03 15:48:25,579 - functions - INFO - Translation pipeline completed successfully
2024-12-03 15:48:25,579 - functions - INFO - === System Information ===
2024-12-03 15:48:25,579 - functions - INFO - PyTorch version: 2.5.0+cu124
2024-12-03 15:48:25,579 - functions - INFO - CUDA available: True
2024-12-03 15:48:25,580 - functions - INFO - Current device: 0
2024-12-03 15:48:25,580 - functions - INFO - Log file location: logs/nllb_TWI_ENGLISH_20241203_154825.log
2024-12-03 15:48:25,580 - functions - INFO - ==================================================
2024-12-03 15:48:25,580 - functions - INFO - Initializing nllb translator...
2024-12-03 15:48:25,580 - functions - INFO - === NLLB Translator Configuration ===
2024-12-03 15:48:25,580 - functions - INFO - Model name: facebook/nllb-200-3.3B
2024-12-03 15:48:25,580 - functions - INFO - Max length: 128
2024-12-03 15:48:25,580 - functions - INFO - Batch size: 8
2024-12-03 15:48:25,580 - functions - INFO - Number of epochs: 1
2024-12-03 15:48:25,580 - functions - INFO - Learning rate: 1e-05
2024-12-03 15:48:25,580 - functions - INFO - Weight decay: 0.01
2024-12-03 15:48:25,580 - functions - INFO - Output directory: ./nllb_TWI_ENGLISH_20241203_154825
2024-12-03 15:48:25,580 - functions - INFO - Source language: twi_Latn
2024-12-03 15:48:25,580 - functions - INFO - Target language: eng_Latn
2024-12-03 15:48:25,580 - functions - INFO - Device: cuda
2024-12-03 15:48:25,580 - functions - INFO - ==================================================
2024-12-03 15:48:28,624 - functions - INFO - Total parameters: 3,344,863,232
2024-12-03 15:48:28,625 - functions - INFO - Trainable parameters: 3,344,863,232
2024-12-03 15:48:28,625 - functions - INFO - Using 4 GPUs
2024-12-03 15:48:28,625 - functions - INFO - Beginning model training procedures...
2024-12-03 15:49:07,753 - functions - INFO - Starting training...
2024-12-03 15:49:37,182 - functions - ERROR - Error computing metrics: argument 'ids': 'list' object cannot be interpreted as an integer
2024-12-03 15:50:22,994 - functions - INFO - Model saved to ./nllb_TWI_ENGLISH_20241203_154825
2024-12-03 15:50:22,997 - functions - INFO - Starting evaluation...
2024-12-03 15:50:30,818 - functions - ERROR - Error computing metrics: argument 'ids': 'list' object cannot be interpreted as an integer
2024-12-03 15:50:32,945 - functions - INFO - Training completed. Final metrics: {'eval_loss': 12.320920944213867, 'eval_bleu': 0.0, 'eval_runtime': 9.9419, 'eval_samples_per_second': 1.006, 'eval_steps_per_second': 0.201, 'epoch': 1.0}
2024-12-03 15:50:32,945 - functions - INFO - Beginning model evaluation...
2024-12-03 15:50:32,947 - functions - INFO - Generating translations for test sample...
2024-12-03 15:50:33,535 - functions - INFO - Translation results saved to ./nllb_TWI_ENGLISH_20241203_154825/sample_translations.csv
2024-12-03 15:50:33,535 - functions - INFO - Metrics saved to ./nllb_TWI_ENGLISH_20241203_154825/sample_metrics.csv
2024-12-03 15:50:33,535 - functions - INFO - 
Example Translations:
2024-12-03 15:50:33,535 - functions - INFO - --------------------------------------------------
2024-12-03 15:50:33,535 - functions - INFO - Source: Wɔrentumi mfa Ofir sikakɔkɔɔ ntɔ apopobibirieboɔ anaa aboɔdemmoɔ nso saa ara
2024-12-03 15:50:33,536 - functions - INFO - Reference: It is not valued with pure gold of Ophir, With precious onyx and sapphire
2024-12-03 15:50:33,536 - functions - INFO - Translation: Neither can the topaz be bought with gold of Ophir, nor the sapphire with gold of Ophir;
2024-12-03 15:50:33,536 - functions - INFO - BLEU Score: 0.14
2024-12-03 15:50:33,536 - functions - INFO - --------------------------------------------------
2024-12-03 15:50:33,536 - functions - INFO - Source: “Afei, ɛsiane sɛ Honhom Kronkron ahyɛ me no enti, merekɔ Yerusalem a mennim asɛm a ɛbɛto me wɔ hɔ
2024-12-03 15:50:33,536 - functions - INFO - Reference: And now, behold, I—bound in the Spirit—go on to Jerusalem, not knowing the things that will befall me in it
2024-12-03 15:50:33,536 - functions - INFO - Translation: "And now, compelled by the Spirit, I am going to Jerusalem, not knowing what will happen to me there.
2024-12-03 15:50:33,536 - functions - INFO - BLEU Score: 0.22
2024-12-03 15:50:33,536 - functions - INFO - --------------------------------------------------
2024-12-03 15:50:33,536 - functions - INFO - 
Aggregate Metrics:
2024-12-03 15:50:33,536 - functions - INFO - average_bleu: 0.18
2024-12-03 15:50:33,536 - functions - INFO - max_bleu: 0.22
2024-12-03 15:50:33,536 - functions - INFO - min_bleu: 0.14
2024-12-03 15:50:33,536 - functions - INFO - num_samples: 2
2024-12-03 15:50:33,536 - functions - INFO - Translation pipeline completed successfully
2024-12-03 15:50:33,537 - functions - INFO - === System Information ===
2024-12-03 15:50:33,537 - functions - INFO - PyTorch version: 2.5.0+cu124
2024-12-03 15:50:33,537 - functions - INFO - CUDA available: True
2024-12-03 15:50:33,537 - functions - INFO - Current device: 0
2024-12-03 15:50:33,537 - functions - INFO - Log file location: logs/opt_TWI_ENGLISH_20241203_155033.log
2024-12-03 15:50:33,537 - functions - INFO - ==================================================
2024-12-03 15:50:33,537 - functions - INFO - Initializing opt translator...
2024-12-03 15:50:34,898 - functions - INFO - === OPT Translator Configuration ===
2024-12-03 15:50:34,899 - functions - INFO - Model name: facebook/opt-350m
2024-12-03 15:50:34,899 - functions - INFO - Max length: 128
2024-12-03 15:50:34,899 - functions - INFO - Batch size: 8
2024-12-03 15:50:34,899 - functions - INFO - Number of epochs: 1
2024-12-03 15:50:34,899 - functions - INFO - Learning rate: 1e-05
2024-12-03 15:50:34,899 - functions - INFO - Weight decay: 0.01
2024-12-03 15:50:34,899 - functions - INFO - Source language: TWI
2024-12-03 15:50:34,899 - functions - INFO - Target language: ENGLISH
2024-12-03 15:50:34,899 - functions - INFO - Device: cuda
2024-12-03 15:50:34,901 - functions - INFO - Total parameters: 331,196,416
2024-12-03 15:50:34,901 - functions - INFO - Trainable parameters: 331,196,416
2024-12-03 15:50:34,901 - functions - INFO - ==================================================
2024-12-03 15:50:34,901 - functions - INFO - Beginning model training procedures...
2024-12-03 15:50:36,763 - functions - INFO - Starting training...
2024-12-03 15:50:42,019 - functions - INFO - BLEU score: 0.0000
2024-12-03 15:50:47,511 - functions - INFO - BLEU score: 0.0000
2024-12-03 15:50:55,073 - functions - INFO - BLEU score: 0.0000
2024-12-03 15:51:00,432 - functions - INFO - Model saved to ./opt_TWI_ENGLISH_20241203_155033
2024-12-03 15:51:01,359 - functions - INFO - BLEU score: 0.0000
2024-12-03 15:51:01,362 - functions - INFO - Training completed. Final metrics: {'eval_loss': 8.425249099731445, 'eval_bleu': 0.0, 'eval_runtime': 0.9257, 'eval_samples_per_second': 10.802, 'eval_steps_per_second': 2.16, 'epoch': 3.0}
2024-12-03 15:51:01,363 - functions - INFO - Beginning model evaluation...
2024-12-03 15:51:01,366 - functions - INFO - Generating translations for test sample...
2024-12-03 15:51:02,249 - functions - INFO - Translation results saved to ./opt_TWI_ENGLISH_20241203_155033/sample_translations.csv
2024-12-03 15:51:02,250 - functions - INFO - Metrics saved to ./opt_TWI_ENGLISH_20241203_155033/sample_metrics.csv
2024-12-03 15:51:02,250 - functions - INFO - 
Example Translations:
2024-12-03 15:51:02,250 - functions - INFO - --------------------------------------------------
2024-12-03 15:51:02,250 - functions - INFO - Source: Wɔrentumi mfa Ofir sikakɔkɔɔ ntɔ apopobibirieboɔ anaa aboɔdemmoɔ nso saa ara
2024-12-03 15:51:02,250 - functions - INFO - Reference: It is not valued with pure gold of Ophir, With precious onyx and sapphire
2024-12-03 15:51:02,250 - functions - INFO - Translation: >Wɔntumi Mfa ofir sikaɔ kɔbɔ bɔpɔ tɔnɔ pɔmɔ wɔrɔ rɔwɔ dɔ
2024-12-03 15:51:02,250 - functions - INFO - BLEU Score: 0.00
2024-12-03 15:51:02,250 - functions - INFO - --------------------------------------------------
2024-12-03 15:51:02,250 - functions - INFO - Source: “Afei, ɛsiane sɛ Honhom Kronkron ahyɛ me no enti, merekɔ Yerusalem a mennim asɛm a ɛbɛto me wɔ hɔ
2024-12-03 15:51:02,250 - functions - INFO - Reference: And now, behold, I—bound in the Spirit—go on to Jerusalem, not knowing the things that will befall me in it
2024-12-03 15:51:02,250 - functions - INFO - Translation: I’m not sure what you’re trying to say here, but it’s pretty clear that you don’t know how to speak English.
You're right, I didn't
2024-12-03 15:51:02,250 - functions - INFO - BLEU Score: 0.00
2024-12-03 15:51:02,250 - functions - INFO - --------------------------------------------------
2024-12-03 15:51:02,250 - functions - INFO - 
Aggregate Metrics:
2024-12-03 15:51:02,250 - functions - INFO - average_bleu: 0.00
2024-12-03 15:51:02,250 - functions - INFO - max_bleu: 0.00
2024-12-03 15:51:02,251 - functions - INFO - min_bleu: 0.00
2024-12-03 15:51:02,251 - functions - INFO - num_samples: 2
2024-12-03 15:51:02,251 - functions - INFO - Translation pipeline completed successfully
2024-12-03 15:51:02,251 - functions - INFO - === System Information ===
2024-12-03 15:51:02,251 - functions - INFO - PyTorch version: 2.5.0+cu124
2024-12-03 15:51:02,251 - functions - INFO - CUDA available: True
2024-12-03 15:51:02,251 - functions - INFO - Current device: 0
2024-12-03 15:51:02,251 - functions - INFO - Log file location: logs/xglm_TWI_ENGLISH_20241203_155102.log
2024-12-03 15:51:02,251 - functions - INFO - ==================================================
2024-12-03 15:51:02,251 - functions - INFO - Initializing xglm translator...
2024-12-03 15:51:06,670 - functions - INFO - === XGLM Translator Configuration ===
2024-12-03 15:51:06,670 - functions - INFO - Model name: facebook/xglm-564M
2024-12-03 15:51:06,670 - functions - INFO - Max length: 128
2024-12-03 15:51:06,670 - functions - INFO - Batch size: 8
2024-12-03 15:51:06,670 - functions - INFO - Number of epochs: 1
2024-12-03 15:51:06,670 - functions - INFO - Learning rate: 1e-05
2024-12-03 15:51:06,670 - functions - INFO - Weight decay: 0.01
2024-12-03 15:51:06,670 - functions - INFO - Source language: TWI
2024-12-03 15:51:06,670 - functions - INFO - Target language: ENGLISH
2024-12-03 15:51:06,670 - functions - INFO - Device: cuda
2024-12-03 15:51:06,672 - functions - INFO - Total parameters: 564,463,616
2024-12-03 15:51:06,673 - functions - INFO - Trainable parameters: 564,463,616
2024-12-03 15:51:06,673 - functions - INFO - ==================================================
2024-12-03 15:51:06,674 - functions - INFO - Beginning model training procedures...
2024-12-03 15:51:09,355 - functions - INFO - Starting training...
2024-12-03 15:51:18,270 - functions - INFO - BLEU score: 0.0000
2024-12-03 15:51:29,710 - functions - INFO - BLEU score: 0.0000
2024-12-03 15:51:43,188 - functions - INFO - BLEU score: 0.0000
2024-12-03 15:51:52,528 - functions - INFO - Model saved to ./xglm_TWI_ENGLISH_20241203_155102
2024-12-03 15:51:55,932 - functions - INFO - BLEU score: 0.0000
2024-12-03 15:51:56,262 - functions - INFO - Training completed. Final metrics: {'eval_loss': 11.270589828491211, 'eval_bleu': 0.0, 'eval_runtime': 3.4028, 'eval_samples_per_second': 2.939, 'eval_steps_per_second': 0.588, 'epoch': 3.0}
2024-12-03 15:51:56,262 - functions - INFO - Beginning model evaluation...
2024-12-03 15:51:56,264 - functions - INFO - Generating translations for test sample...
2024-12-03 15:51:56,960 - functions - INFO - Translation results saved to ./xglm_TWI_ENGLISH_20241203_155102/sample_translations.csv
2024-12-03 15:51:56,960 - functions - INFO - Metrics saved to ./xglm_TWI_ENGLISH_20241203_155102/sample_metrics.csv
2024-12-03 15:51:56,960 - functions - INFO - 
Example Translations:
2024-12-03 15:51:56,960 - functions - INFO - --------------------------------------------------
2024-12-03 15:51:56,960 - functions - INFO - Source: Wɔrentumi mfa Ofir sikakɔkɔɔ ntɔ apopobibirieboɔ anaa aboɔdemmoɔ nso saa ara
2024-12-03 15:51:56,960 - functions - INFO - Reference: It is not valued with pure gold of Ophir, With precious onyx and sapphire
2024-12-03 15:51:56,960 - functions - INFO - Translation: I'm sorry, but I don't understand what you're trying to say.
2024-12-03 15:51:56,960 - functions - INFO - BLEU Score: 0.00
2024-12-03 15:51:56,960 - functions - INFO - --------------------------------------------------
2024-12-03 15:51:56,960 - functions - INFO - Source: “Afei, ɛsiane sɛ Honhom Kronkron ahyɛ me no enti, merekɔ Yerusalem a mennim asɛm a ɛbɛto me wɔ hɔ
2024-12-03 15:51:56,961 - functions - INFO - Reference: And now, behold, I—bound in the Spirit—go on to Jerusalem, not knowing the things that will befall me in it
2024-12-03 15:51:56,961 - functions - INFO - Translation: I’m not sure what you mean by that, but it sounds like you’re talking about something else.”
2024-12-03 15:51:56,961 - functions - INFO - BLEU Score: 0.00
2024-12-03 15:51:56,961 - functions - INFO - --------------------------------------------------
2024-12-03 15:51:56,961 - functions - INFO - 
Aggregate Metrics:
2024-12-03 15:51:56,961 - functions - INFO - average_bleu: 0.00
2024-12-03 15:51:56,961 - functions - INFO - max_bleu: 0.00
2024-12-03 15:51:56,961 - functions - INFO - min_bleu: 0.00
2024-12-03 15:51:56,961 - functions - INFO - num_samples: 2
2024-12-03 15:51:56,961 - functions - INFO - Translation pipeline completed successfully
2024-12-03 15:51:56,961 - functions - INFO - === System Information ===
2024-12-03 15:51:56,961 - functions - INFO - PyTorch version: 2.5.0+cu124
2024-12-03 15:51:56,962 - functions - INFO - CUDA available: True
2024-12-03 15:51:56,962 - functions - INFO - Current device: 0
2024-12-03 15:51:56,962 - functions - INFO - Log file location: logs/falcon_TWI_ENGLISH_20241203_155156.log
2024-12-03 15:51:56,962 - functions - INFO - ==================================================
2024-12-03 15:51:56,962 - functions - INFO - Initializing falcon translator...
2024-12-03 15:51:57,403 - transformers_modules.tiiuae.falcon-7b.ec89142b67d748a1865ea4451372db8313ada0d8.configuration_falcon - WARNING - 
WARNING: You are currently loading Falcon using legacy code contained in the model repository. Falcon has now been fully ported into the Hugging Face transformers library. For the most up-to-date and high-performance version of the Falcon model code, please update to the latest version of transformers and then load the model without the trust_remote_code=True argument.

2024-12-03 15:52:02,978 - functions - INFO - === Falcon Translator Configuration ===
2024-12-03 15:52:02,979 - functions - INFO - Model name: tiiuae/falcon-7b
2024-12-03 15:52:02,979 - functions - INFO - Max length: 128
2024-12-03 15:52:02,979 - functions - INFO - Batch size: 8
2024-12-03 15:52:02,979 - functions - INFO - Number of epochs: 1
2024-12-03 15:52:02,979 - functions - INFO - Learning rate: 1e-05
2024-12-03 15:52:02,979 - functions - INFO - Weight decay: 0.01
2024-12-03 15:52:02,979 - functions - INFO - Source language: TWI
2024-12-03 15:52:02,979 - functions - INFO - Target language: ENGLISH
2024-12-03 15:52:02,979 - functions - INFO - Device: cuda
2024-12-03 15:52:02,981 - functions - INFO - Total parameters: 6,921,720,704
2024-12-03 15:52:02,981 - functions - INFO - Trainable parameters: 6,921,720,704
2024-12-03 15:52:02,981 - functions - INFO - ==================================================
2024-12-03 15:52:02,983 - functions - INFO - Beginning model training procedures...
2024-12-03 15:53:33,991 - functions - INFO - Starting training...
2024-12-03 15:53:37,374 - functions - ERROR - Error during training: Caught OutOfMemoryError in replica 0 on device 0.
Original Traceback (most recent call last):
  File "/home/sheriff/miniconda3/envs/llm_loc/lib/python3.12/site-packages/torch/nn/parallel/parallel_apply.py", line 96, in _worker
    output = module(*input, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/sheriff/miniconda3/envs/llm_loc/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/sheriff/miniconda3/envs/llm_loc/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/sheriff/.cache/huggingface/modules/transformers_modules/tiiuae/falcon-7b/ec89142b67d748a1865ea4451372db8313ada0d8/modeling_falcon.py", line 900, in forward
    transformer_outputs = self.transformer(
                          ^^^^^^^^^^^^^^^^^
  File "/home/sheriff/miniconda3/envs/llm_loc/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/sheriff/miniconda3/envs/llm_loc/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/sheriff/.cache/huggingface/modules/transformers_modules/tiiuae/falcon-7b/ec89142b67d748a1865ea4451372db8313ada0d8/modeling_falcon.py", line 797, in forward
    outputs = block(
              ^^^^^^
  File "/home/sheriff/miniconda3/envs/llm_loc/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/sheriff/miniconda3/envs/llm_loc/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/sheriff/.cache/huggingface/modules/transformers_modules/tiiuae/falcon-7b/ec89142b67d748a1865ea4451372db8313ada0d8/modeling_falcon.py", line 453, in forward
    attn_outputs = self.self_attention(
                   ^^^^^^^^^^^^^^^^^^^^
  File "/home/sheriff/miniconda3/envs/llm_loc/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/sheriff/miniconda3/envs/llm_loc/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/sheriff/.cache/huggingface/modules/transformers_modules/tiiuae/falcon-7b/ec89142b67d748a1865ea4451372db8313ada0d8/modeling_falcon.py", line 341, in forward
    attn_output = F.scaled_dot_product_attention(
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 568.00 MiB. GPU 0 has a total capacity of 93.12 GiB of which 270.00 MiB is free. Process 1554694 has 7.23 GiB memory in use. Process 1716044 has 8.51 GiB memory in use. Including non-PyTorch memory, this process has 77.09 GiB memory in use. Of the allocated memory 74.34 GiB is allocated by PyTorch, and 1.33 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

2024-12-03 15:53:37,374 - functions - ERROR - Error in falcon pipeline: Caught OutOfMemoryError in replica 0 on device 0.
Original Traceback (most recent call last):
  File "/home/sheriff/miniconda3/envs/llm_loc/lib/python3.12/site-packages/torch/nn/parallel/parallel_apply.py", line 96, in _worker
    output = module(*input, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/sheriff/miniconda3/envs/llm_loc/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/sheriff/miniconda3/envs/llm_loc/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/sheriff/.cache/huggingface/modules/transformers_modules/tiiuae/falcon-7b/ec89142b67d748a1865ea4451372db8313ada0d8/modeling_falcon.py", line 900, in forward
    transformer_outputs = self.transformer(
                          ^^^^^^^^^^^^^^^^^
  File "/home/sheriff/miniconda3/envs/llm_loc/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/sheriff/miniconda3/envs/llm_loc/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/sheriff/.cache/huggingface/modules/transformers_modules/tiiuae/falcon-7b/ec89142b67d748a1865ea4451372db8313ada0d8/modeling_falcon.py", line 797, in forward
    outputs = block(
              ^^^^^^
  File "/home/sheriff/miniconda3/envs/llm_loc/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/sheriff/miniconda3/envs/llm_loc/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/sheriff/.cache/huggingface/modules/transformers_modules/tiiuae/falcon-7b/ec89142b67d748a1865ea4451372db8313ada0d8/modeling_falcon.py", line 453, in forward
    attn_outputs = self.self_attention(
                   ^^^^^^^^^^^^^^^^^^^^
  File "/home/sheriff/miniconda3/envs/llm_loc/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/sheriff/miniconda3/envs/llm_loc/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/sheriff/.cache/huggingface/modules/transformers_modules/tiiuae/falcon-7b/ec89142b67d748a1865ea4451372db8313ada0d8/modeling_falcon.py", line 341, in forward
    attn_output = F.scaled_dot_product_attention(
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 568.00 MiB. GPU 0 has a total capacity of 93.12 GiB of which 270.00 MiB is free. Process 1554694 has 7.23 GiB memory in use. Process 1716044 has 8.51 GiB memory in use. Including non-PyTorch memory, this process has 77.09 GiB memory in use. Of the allocated memory 74.34 GiB is allocated by PyTorch, and 1.33 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Traceback (most recent call last):
  File "/home/sheriff/research/twi/unified_model_trainer.py", line 45, in run_translation_pipeline
    metrics = translator.train(dataset)
              ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/sheriff/research/twi/functions.py", line 2247, in train
    trainer.train()
  File "/home/sheriff/miniconda3/envs/llm_loc/lib/python3.12/site-packages/transformers/trainer.py", line 2052, in train
    return inner_training_loop(
           ^^^^^^^^^^^^^^^^^^^^
  File "/home/sheriff/miniconda3/envs/llm_loc/lib/python3.12/site-packages/transformers/trainer.py", line 2388, in _inner_training_loop
    tr_loss_step = self.training_step(model, inputs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/sheriff/miniconda3/envs/llm_loc/lib/python3.12/site-packages/transformers/trainer.py", line 3485, in training_step
    loss = self.compute_loss(model, inputs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/sheriff/miniconda3/envs/llm_loc/lib/python3.12/site-packages/transformers/trainer.py", line 3532, in compute_loss
    outputs = model(**inputs)
              ^^^^^^^^^^^^^^^
  File "/home/sheriff/miniconda3/envs/llm_loc/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/sheriff/miniconda3/envs/llm_loc/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/sheriff/miniconda3/envs/llm_loc/lib/python3.12/site-packages/torch/nn/parallel/data_parallel.py", line 193, in forward
    outputs = self.parallel_apply(replicas, inputs, module_kwargs)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/sheriff/miniconda3/envs/llm_loc/lib/python3.12/site-packages/torch/nn/parallel/data_parallel.py", line 212, in parallel_apply
    return parallel_apply(
           ^^^^^^^^^^^^^^^
  File "/home/sheriff/miniconda3/envs/llm_loc/lib/python3.12/site-packages/torch/nn/parallel/parallel_apply.py", line 126, in parallel_apply
    output.reraise()
  File "/home/sheriff/miniconda3/envs/llm_loc/lib/python3.12/site-packages/torch/_utils.py", line 715, in reraise
    raise exception
torch.OutOfMemoryError: Caught OutOfMemoryError in replica 0 on device 0.
Original Traceback (most recent call last):
  File "/home/sheriff/miniconda3/envs/llm_loc/lib/python3.12/site-packages/torch/nn/parallel/parallel_apply.py", line 96, in _worker
    output = module(*input, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/sheriff/miniconda3/envs/llm_loc/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/sheriff/miniconda3/envs/llm_loc/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/sheriff/.cache/huggingface/modules/transformers_modules/tiiuae/falcon-7b/ec89142b67d748a1865ea4451372db8313ada0d8/modeling_falcon.py", line 900, in forward
    transformer_outputs = self.transformer(
                          ^^^^^^^^^^^^^^^^^
  File "/home/sheriff/miniconda3/envs/llm_loc/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/sheriff/miniconda3/envs/llm_loc/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/sheriff/.cache/huggingface/modules/transformers_modules/tiiuae/falcon-7b/ec89142b67d748a1865ea4451372db8313ada0d8/modeling_falcon.py", line 797, in forward
    outputs = block(
              ^^^^^^
  File "/home/sheriff/miniconda3/envs/llm_loc/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/sheriff/miniconda3/envs/llm_loc/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/sheriff/.cache/huggingface/modules/transformers_modules/tiiuae/falcon-7b/ec89142b67d748a1865ea4451372db8313ada0d8/modeling_falcon.py", line 453, in forward
    attn_outputs = self.self_attention(
                   ^^^^^^^^^^^^^^^^^^^^
  File "/home/sheriff/miniconda3/envs/llm_loc/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/sheriff/miniconda3/envs/llm_loc/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/sheriff/.cache/huggingface/modules/transformers_modules/tiiuae/falcon-7b/ec89142b67d748a1865ea4451372db8313ada0d8/modeling_falcon.py", line 341, in forward
    attn_output = F.scaled_dot_product_attention(
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 568.00 MiB. GPU 0 has a total capacity of 93.12 GiB of which 270.00 MiB is free. Process 1554694 has 7.23 GiB memory in use. Process 1716044 has 8.51 GiB memory in use. Including non-PyTorch memory, this process has 77.09 GiB memory in use. Of the allocated memory 74.34 GiB is allocated by PyTorch, and 1.33 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

